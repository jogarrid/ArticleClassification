{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1stMLmodel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wBhG_9TUq66U",
        "colab_type": "code",
        "outputId": "29ecf29d-eeac-4252-a740-e012c577535e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import numpy as np\n",
        "!pip install gensim\n",
        "import gensim\n",
        "import logging\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 15.5MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/ab/de8fc0a6f21b2ecba1e78d86afbeba89171ef06fc77f63b3d7b4e8cf62ee/boto3-1.9.58-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 29.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.3MB/s \n",
            "\u001b[?25hCollecting botocore<1.13.0,>=1.12.58 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/f0/e1640f8aa77006d0261db35d80eb995668f3de5439ea2ff624b44a176852/botocore-1.12.58-py2.py3-none-any.whl (5.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.1MB 6.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.58->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.58->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 24.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.58 botocore-1.12.58 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_VI9sOOysWUd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BKvDCQDLtzFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load Google's pre-trained Word2Vec model.\n",
        "model =gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJHWP6RVsVKv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dismissed = pd.read_csv('/content/gdrive/My Drive/MLdata/dismissed_complete.csv')\n",
        "non_dismissed = pd.read_csv('/content/gdrive/My Drive/MLdata/nodismissed_complete.csv')\n",
        "\n",
        "#Delete random columns to have same number of dimissed and non dismissed\n",
        "\n",
        "indexes_drop = np.arange(len(non_dismissed)-len(dismissed))\n",
        "np.random.shuffle(indexes_drop)\n",
        "non_dismissed = non_dismissed.drop(indexes_drop, axis = 0)\n",
        "\n",
        "index = list(range(len(dismissed), len(dismissed)+len(non_dismissed)))\n",
        "\n",
        "#non_dismissed.set_index(range(0,len(non_dismissed)+len(dismissed)))\n",
        "all_authors = pd.concat((dismissed, non_dismissed))\n",
        "\n",
        "index = range(0,len(all_authors))\n",
        "all_authors['index'] = index\n",
        "all_authors = all_authors.set_index('index')\n",
        "\n",
        "#all_authors['Title paper']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m1YuZY_T2PH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d948abf6-bcb2-43ab-bd9f-e7522201826c"
      },
      "cell_type": "code",
      "source": [
        "len(dismissed)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12301"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "metadata": {
        "id": "24cUB_9ttNai",
        "colab_type": "code",
        "outputId": "321ff671-4495-4bd6-e04e-4b4dcef1bb7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#we obtain a vector associated to each paper\n",
        "vecs = {\"vec_title\": [], \"vec_journal\":[]}\n",
        "nans = 0 \n",
        "for i in range(len(all_authors)):\n",
        "  #title is a list of words\n",
        "  title =all_authors['Title paper'][i][1:(len(dismissed['Title paper'][9])-1)].split(' ')\n",
        "  \n",
        "  #300 is the shape of the vector representation of a word in the google word2vec model\n",
        "  vec_title = np.zeros((300))\n",
        "  count = 0\n",
        "  for j in range(len(title)):\n",
        "    word = title[j]\n",
        "    if (word in model): \n",
        "      vec_title += model[title[j]]\n",
        "      count += 1\n",
        "    \n",
        "  #Normalize to account for the fact that title's aren't exactly the same length\n",
        "  vec_title = vec_title / count\n",
        "\n",
        "  \n",
        "  #obtain a vec representation of the journal\n",
        "  #list of words\n",
        "  journal = all_authors['Journal'][9][1:(len(all_authors['Journal'][9])-1)].split(' ')\n",
        "  \n",
        "  #300 is the shape of the vector representation of a word in the google word2vec model\n",
        "  vec_journal = np.zeros((300))\n",
        "  count = 0\n",
        "  for j in range(len(journal)):\n",
        "    word = journal[j]\n",
        "    if(word in model): \n",
        "      vec_journal += model[journal[j]]\n",
        "      count += 1\n",
        "    \n",
        "  #Normalize to account for different lengths\n",
        "  vec_journal= vec_journal / count\n",
        "\n",
        "  #Save vectors and data for this paper\n",
        "  vecs['vec_title'].append(vec_title)\n",
        "  vecs['vec_journal'].append(vec_journal)\n",
        "\n",
        "all_authors['vec_title']  =  vecs['vec_title']\n",
        "all_authors['vec_journal'] =  vecs['vec_journal']\n",
        "all_authors_or = all_authors.copy()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-GjRXqd2h2f8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inter = all_authors['vec_title'].as_matrix()\n",
        "bools = [np.any(np.isnan(vec)) for vec in inter]\n",
        "indexes_nan = np.arange(len(bools))\n",
        "indexes_nan = indexes_nan[bools]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wz0oeGO3YB46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#delete rows that contain nan elements\n",
        "all_authors = all_authors_or\n",
        "all_authors = all_authors.drop(indexes_nan, axis = 0)\n",
        "indexes = np.arange(len(all_authors))\n",
        "all_authors['index']  = indexes\n",
        "all_authors = all_authors.set_index('index')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bx1_q1iGuHVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#shuffle data for training\n",
        "indexes = np.arange(len(all_authors))\n",
        "np.random.shuffle(indexes)\n",
        "all_authors['index']  = indexes\n",
        "all_authors = all_authors.set_index('index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1zMypAIkaJja",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ix_train = range(0, round(0.7*len(all_authors)))\n",
        "ix_test = range(len(ix_train), len(all_authors))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UenGeFXQZ1pA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class Model():\n",
        "  def __init__(self, batch_size, learning_rate, iterations):\n",
        "    super(Model, self).__init__()\n",
        "    n_output = 2     #this model does classification. There are 2 classes \n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.graph = tf.Graph()\n",
        "    N = [200, 100, 15] #size of hidden layers\n",
        "    with self.graph.as_default():\n",
        "\n",
        "      self.h1_weights=tf.Variable(tf.random_normal([300,N[0]]))\n",
        "      self.h1_bias=tf.Variable(tf.random_normal([N[0]]))\n",
        "      \n",
        "      self.h2_weights=tf.Variable(tf.random_normal([N[0], N[1]]))\n",
        "      self.h2_bias=tf.Variable(tf.random_normal([N[1]]))\n",
        "      \n",
        "      self.h3_weights=tf.Variable(tf.random_normal([N[1], N[2]]))\n",
        "      self.h3_bias=tf.Variable(tf.random_normal([N[2]]))\n",
        "      \n",
        "      self.out_weights=tf.Variable(tf.random_normal([N[2], 2]))\n",
        "      self.out_bias=tf.Variable(tf.random_normal([2]))\n",
        "      \n",
        "\n",
        "      #defining placeholders\n",
        "      #input image placeholder\n",
        "      self.x=tf.placeholder(\"float\",[None,300]) #batch_size x n_input. n_input is 300 as there are 300 parameters\n",
        "      #input label placeholder\n",
        "      self.y=tf.placeholder(\"int32\",[None])\n",
        "      \n",
        "      self.y_ = tf.one_hot(self.y, 2)\n",
        "\n",
        "      #processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
        "      #self.input=tf.unstack(self.x ,time_steps, 1)\n",
        "      \n",
        "      \n",
        "      #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
        "      #self.prediction=tf.matmul(self.outputs, self.out_weights)+self.out_bias\n",
        "\n",
        "      #output of 1st hidden layer (hidden1) is I1\n",
        "      I1 = tf.nn.relu(tf.nn.xw_plus_b(self.x, self.h1_weights, self.h1_bias, name=\"hidden1\"))\n",
        "      I2 = tf.nn.relu(tf.nn.xw_plus_b(I1, self.h2_weights, self.h2_bias, name=\"hidden2\"))\n",
        "      I3 = tf.nn.relu(tf.nn.xw_plus_b(I2, self.h3_weights, self.h3_bias, name=\"hidden3\"))\n",
        "  \n",
        "      self.logits = tf.nn.relu(tf.nn.xw_plus_b(I3, self.out_weights, self.out_bias, name=\"output\"))\n",
        "\n",
        "      self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n",
        "\n",
        "      #Calculate mean cross-entropy loss\n",
        "      self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_))\n",
        "      \n",
        "      self.acc = tf.metrics.accuracy(self.predictions, self.y)\n",
        "      #self.acc = 0.9\n",
        "      #optimization\n",
        "      self.opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
        "\n",
        "      \n",
        "  def train(self, fx_divided, labels, directory):\n",
        "    #fx_divided needs to be  an array of shape samples,  2, time_steps\n",
        "    self.directory = directory\n",
        "    losst = 0\n",
        "    with self.graph.as_default():\n",
        "      \n",
        "\n",
        "      self.saver = tf.train.Saver()\n",
        "      with tf.Session() as sess:\n",
        "        #sess.run(init)\n",
        "        sess.run(tf.local_variables_initializer())\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        for iter in range(iterations):\n",
        "          indexesBatch = np.random.randint(0, fx_divided.shape[0], self.batch_size)\n",
        "          #batch_y = y[indexesBatch]\n",
        "          batch = np.zeros((self.batch_size, 300))  #there are 10 time_steps \n",
        "          #batch_y = np.zeros((self.batch_size, self.time_steps, 2))  #there are 10 time_steps \n",
        "\n",
        "          batch = fx_divided[indexesBatch]\n",
        "          batch_y = labels[indexesBatch]\n",
        "          print(batch.shape)\n",
        "          print(batch_y.shape)\n",
        "          \n",
        "          _, los, acc = sess.run([self.opt, self.loss, self.acc],feed_dict={self.x: batch, self.y: batch_y})\n",
        "          losst += los\n",
        "          print(\"For iter \",iter)\n",
        "          print(\"Loss\",losst)\n",
        "          print(\"accuracy: \", acc)\n",
        "          losst = 0\n",
        "\n",
        "        self.save_path = self.saver.save(sess, '/content/gdrive/My Drive/model/' + self.directory)\n",
        "        print(self.save_path)\n",
        " \n",
        "          \n",
        "  def predict(self, inputt):\n",
        "    #This code is very slow as you have to access two graphs, for x and for y, before predicting each sample, and to access each model you need to read a file from disk\n",
        "    with self.graph.as_default():   \n",
        "      with tf.Session() as sess:\n",
        "        self.saver.restore(sess, self.save_path)  \n",
        "        inputt = inputt.reshape(1, 300)\n",
        "        pred = sess.run([self.predictions], feed_dict={self.x: inputt})\n",
        " \n",
        "    return pred\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2_w9S3ct8-l",
        "colab_type": "code",
        "outputId": "1a36c993-4584-426e-da8b-72d36b1892f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45035
        }
      },
      "cell_type": "code",
      "source": [
        "iterations = 500\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_data  = all_authors['vec_title'][list(ix_train)].as_matrix()\n",
        "labels = np.array(all_authors['labels'][list(ix_train)].as_matrix())\n",
        "\n",
        "train_data = np.array([list(train_data[i]) for i in range(len(train_data))])\n",
        "#Create and train models#\n",
        "modelx = Model(batch_size, learning_rate, iterations)\n",
        "\n",
        "lossf = modelx.train(train_data, labels, 'modelx1') #Using samples 0-3, we predict sample 4\n",
        "\n"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 300)\n",
            "(50,)\n",
            "For iter  0\n",
            "Loss 110.59333038330078\n",
            "accuracy:  (0.0, 0.44)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  1\n",
            "Loss 73.43575286865234\n",
            "accuracy:  (0.44, 0.49)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  2\n",
            "Loss 50.176937103271484\n",
            "accuracy:  (0.49, 0.52)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  3\n",
            "Loss 72.80352783203125\n",
            "accuracy:  (0.52, 0.53)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  4\n",
            "Loss 36.844459533691406\n",
            "accuracy:  (0.53, 0.52)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  5\n",
            "Loss 51.2374153137207\n",
            "accuracy:  (0.52, 0.49666667)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  6\n",
            "Loss 22.219432830810547\n",
            "accuracy:  (0.49666667, 0.49714285)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  7\n",
            "Loss 32.735023498535156\n",
            "accuracy:  (0.49714285, 0.495)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  8\n",
            "Loss 28.980131149291992\n",
            "accuracy:  (0.495, 0.4977778)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  9\n",
            "Loss 21.08306121826172\n",
            "accuracy:  (0.4977778, 0.496)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  10\n",
            "Loss 6.940021514892578\n",
            "accuracy:  (0.496, 0.5)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  11\n",
            "Loss 9.446808815002441\n",
            "accuracy:  (0.5, 0.49833333)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  12\n",
            "Loss 3.7701938152313232\n",
            "accuracy:  (0.49833333, 0.50769234)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  13\n",
            "Loss 0.9358254075050354\n",
            "accuracy:  (0.50769234, 0.5057143)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  14\n",
            "Loss 5.1535868644714355\n",
            "accuracy:  (0.5057143, 0.508)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  15\n",
            "Loss 3.780224084854126\n",
            "accuracy:  (0.508, 0.5075)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  16\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5075, 0.5070588)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  17\n",
            "Loss 0.651559054851532\n",
            "accuracy:  (0.5070588, 0.51222223)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  18\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.51222223, 0.5094737)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  19\n",
            "Loss 1.4070771932601929\n",
            "accuracy:  (0.5094737, 0.507)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  20\n",
            "Loss 0.6654213070869446\n",
            "accuracy:  (0.507, 0.507619)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  21\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.507619, 0.50545454)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  22\n",
            "Loss 1.5605154037475586\n",
            "accuracy:  (0.50545454, 0.5069565)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  23\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.5069565, 0.5058333)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  24\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5058333, 0.4992)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  25\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.4992, 0.49846154)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  26\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49846154, 0.4977778)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  27\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4977778, 0.4942857)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  28\n",
            "Loss 2.9975321292877197\n",
            "accuracy:  (0.4942857, 0.4937931)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  29\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4937931, 0.494)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  30\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.494, 0.49290323)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  31\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49290323, 0.4975)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  32\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4975, 0.4969697)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  33\n",
            "Loss 0.679434597492218\n",
            "accuracy:  (0.4969697, 0.4970588)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  34\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4970588, 0.49314284)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  35\n",
            "Loss 2.795236825942993\n",
            "accuracy:  (0.49314284, 0.49444443)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  36\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49444443, 0.4918919)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  37\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4918919, 0.4942105)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  38\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4942105, 0.494359)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  39\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.494359, 0.495)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  40\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.495, 0.49365854)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  41\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49365854, 0.4947619)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  42\n",
            "Loss 1.344435453414917\n",
            "accuracy:  (0.4947619, 0.49302325)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  43\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49302325, 0.4918182)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  44\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4918182, 0.492)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  45\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.492, 0.49130434)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  46\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49130434, 0.49021277)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  47\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49021277, 0.49208334)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  48\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49208334, 0.49183673)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  49\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49183673, 0.4924)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  50\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4924, 0.4917647)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  51\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4917647, 0.49307692)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  52\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49307692, 0.4935849)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  53\n",
            "Loss 3.757356882095337\n",
            "accuracy:  (0.4935849, 0.49333334)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  54\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49333334, 0.4941818)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  55\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4941818, 0.4942857)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  56\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4942857, 0.49438596)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  57\n",
            "Loss 2.0055553913116455\n",
            "accuracy:  (0.49438596, 0.4951724)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  58\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4951724, 0.49525425)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  59\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49525425, 0.49466667)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  60\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49466667, 0.49344262)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  61\n",
            "Loss 1.337947964668274\n",
            "accuracy:  (0.49344262, 0.49387097)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  62\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49387097, 0.4942857)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  63\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4942857, 0.495)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  64\n",
            "Loss 0.6792843341827393\n",
            "accuracy:  (0.495, 0.496)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  65\n",
            "Loss 1.9447150230407715\n",
            "accuracy:  (0.496, 0.4960606)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  66\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4960606, 0.49492538)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  67\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49492538, 0.4932353)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  68\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4932353, 0.49507245)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  69\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49507245, 0.49485713)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  70\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49485713, 0.4940845)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  71\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4940845, 0.49555555)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  72\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49555555, 0.49534246)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  73\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49534246, 0.49567568)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  74\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49567568, 0.49493334)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  75\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49493334, 0.49473685)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  76\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49473685, 0.49454546)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  77\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49454546, 0.49461538)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  78\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49461538, 0.4944304)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  79\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4944304, 0.4945)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  80\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4945, 0.49407408)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  81\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49407408, 0.49390244)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  82\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49390244, 0.4959036)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  83\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4959036, 0.4961905)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  84\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4961905, 0.4967059)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  85\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4967059, 0.49627906)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  86\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49627906, 0.49655172)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  87\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49655172, 0.49568182)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  88\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49568182, 0.49483147)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  89\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49483147, 0.49466667)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  90\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49466667, 0.49604395)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  91\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49604395, 0.49608696)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  92\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49608696, 0.4963441)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  93\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4963441, 0.49638298)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  94\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49638298, 0.4951579)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  95\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4951579, 0.495)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  96\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.495, 0.49525774)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  97\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49525774, 0.49591836)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  98\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49591836, 0.4959596)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  99\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4959596, 0.4956)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  100\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4956, 0.4960396)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  101\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4960396, 0.49588236)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  102\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49588236, 0.49553397)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  103\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49553397, 0.49461538)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  104\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49461538, 0.49504763)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  105\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49504763, 0.49490565)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  106\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49490565, 0.49457943)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  107\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49457943, 0.49592593)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  108\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49592593, 0.49522936)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  109\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49522936, 0.49527273)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  110\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49527273, 0.49603602)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  111\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49603602, 0.49607143)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  112\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49607143, 0.49663717)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  113\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49663717, 0.49614036)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  114\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49614036, 0.49617392)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  115\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49617392, 0.4951724)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  116\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4951724, 0.49555555)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  117\n",
            "Loss 0.6804972887039185\n",
            "accuracy:  (0.49555555, 0.4962712)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  118\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4962712, 0.49613446)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  119\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49613446, 0.4975)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  120\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4975, 0.49735537)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  121\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49735537, 0.49639344)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  122\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49639344, 0.49609756)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  123\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49609756, 0.49709678)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  124\n",
            "Loss 2.029550552368164\n",
            "accuracy:  (0.49709678, 0.49792)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  125\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49792, 0.49761903)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  126\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49761903, 0.49795276)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  127\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49795276, 0.49859375)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  128\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49859375, 0.49782947)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  129\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49782947, 0.49723077)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  130\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49723077, 0.49755725)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  131\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49755725, 0.49757576)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  132\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49757576, 0.4956391)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  133\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4956391, 0.49552238)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  134\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49552238, 0.49525926)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  135\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49525926, 0.4947059)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  136\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4947059, 0.49474454)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  137\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49474454, 0.49449274)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  138\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49449274, 0.4943885)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  139\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4943885, 0.49485713)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  140\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49485713, 0.49475178)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  141\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49475178, 0.49394366)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  142\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49394366, 0.49356642)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  143\n",
            "Loss 2.5198276042938232\n",
            "accuracy:  (0.49356642, 0.49416667)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  144\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49416667, 0.49462068)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  145\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49462068, 0.49438357)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  146\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49438357, 0.49537414)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  147\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49537414, 0.49554053)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  148\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49554053, 0.49530202)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  149\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49530202, 0.4952)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  150\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4952, 0.4954967)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  151\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4954967, 0.49578947)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  152\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49578947, 0.49529412)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  153\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49529412, 0.49454546)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  154\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49454546, 0.49419355)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  155\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49419355, 0.49525642)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  156\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49525642, 0.4955414)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  157\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4955414, 0.49544305)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  158\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49544305, 0.4954717)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  159\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4954717, 0.49575)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  160\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49575, 0.49577639)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  161\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49577639, 0.49567902)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  162\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49567902, 0.49644172)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  163\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49644172, 0.4959756)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  164\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4959756, 0.49636364)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  165\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49636364, 0.49626505)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  166\n",
            "Loss 0.7784181833267212\n",
            "accuracy:  (0.49626505, 0.4960479)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  167\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4960479, 0.495)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  168\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.495, 0.49491125)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  169\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49491125, 0.49588236)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  170\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49588236, 0.4962573)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  171\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.4962573, 0.49627906)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  172\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49627906, 0.49606937)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  173\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49606937, 0.49563217)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  174\n",
            "Loss 0.6792842149734497\n",
            "accuracy:  (0.49563217, 0.4953143)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  175\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4953143, 0.49545455)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  176\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49545455, 0.49525425)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  177\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49525425, 0.49505618)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  178\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49505618, 0.49553072)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  179\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49553072, 0.49555555)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  180\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49555555, 0.4951381)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  181\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4951381, 0.49527472)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  182\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49527472, 0.495847)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  183\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.495847, 0.49586958)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  184\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49586958, 0.4958919)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  185\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4958919, 0.4960215)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  186\n",
            "Loss 0.6654213070869446\n",
            "accuracy:  (0.4960215, 0.49625668)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  187\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49625668, 0.49648938)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  188\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49648938, 0.4964021)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  189\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4964021, 0.49694738)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  190\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.49694738, 0.49717277)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  191\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49717277, 0.498125)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  192\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.498125, 0.4984456)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  193\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4984456, 0.4986598)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  194\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4986598, 0.49866667)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  195\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49866667, 0.49836734)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  196\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49836734, 0.49857867)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  197\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49857867, 0.49878788)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  198\n",
            "Loss 2.2857506275177\n",
            "accuracy:  (0.49878788, 0.4992965)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  199\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4992965, 0.4996)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  200\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4996, 0.499602)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  201\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.499602, 0.4992079)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  202\n",
            "Loss 1.7173657417297363\n",
            "accuracy:  (0.4992079, 0.49891627)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  203\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49891627, 0.4987255)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  204\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4987255, 0.49863416)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  205\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49863416, 0.49883494)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  206\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49883494, 0.4989372)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  207\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4989372, 0.49884614)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  208\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49884614, 0.4985646)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  209\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4985646, 0.49847618)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  210\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49847618, 0.49838862)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  211\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49838862, 0.49839622)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  212\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49839622, 0.49887323)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  213\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49887323, 0.49906543)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  214\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49906543, 0.49906978)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  215\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49906978, 0.49935186)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  216\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49935186, 0.49953917)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  217\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49953917, 0.50027525)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  218\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50027525, 0.5007306)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  219\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5007306, 0.50127274)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  220\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50127274, 0.50153846)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  221\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50153846, 0.5016216)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  222\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016216, 0.5013453)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  223\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5013453, 0.5019643)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  224\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5019643, 0.5023111)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  225\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5023111, 0.5020354)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  226\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5020354, 0.502467)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  227\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.502467, 0.5026316)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  228\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5026316, 0.5026201)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  229\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5026201, 0.50252175)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  230\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50252175, 0.50242424)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  231\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50242424, 0.5025)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  232\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5025, 0.5028326)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  233\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5028326, 0.5030769)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  234\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.5030769, 0.50297874)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  235\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50297874, 0.50305086)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  236\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50305086, 0.503038)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  237\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.503038, 0.50285715)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  238\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50285715, 0.5025941)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  239\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5025941, 0.5024167)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  240\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5024167, 0.5025726)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  241\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.5025726, 0.5022314)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  242\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5022314, 0.50263375)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  243\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50263375, 0.50311476)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  244\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50311476, 0.5032653)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  245\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5032653, 0.5030081)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  246\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5030081, 0.50299597)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  247\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50299597, 0.5029032)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  248\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5029032, 0.5029719)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  249\n",
            "Loss 0.6792844533920288\n",
            "accuracy:  (0.5029719, 0.50256)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  250\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50256, 0.50223106)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  251\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50223106, 0.5025397)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  252\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5025397, 0.5022134)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  253\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5022134, 0.502126)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  254\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.502126, 0.5025882)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  255\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5025882, 0.50289065)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  256\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50289065, 0.50241244)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  257\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50241244, 0.5024806)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  258\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5024806, 0.50239384)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  259\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50239384, 0.50276923)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  260\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50276923, 0.50291187)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  261\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50291187, 0.5025954)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  262\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5025954, 0.50258553)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  263\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50258553, 0.50272727)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  264\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50272727, 0.5032453)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  265\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5032453, 0.5038346)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  266\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5038346, 0.5037453)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  267\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5037453, 0.5034328)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  268\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5034328, 0.50349444)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  269\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50349444, 0.5037778)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  270\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5037778, 0.5036162)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  271\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5036162, 0.5036765)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  272\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5036765, 0.50402933)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  273\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50402933, 0.5040146)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  274\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5040146, 0.5040727)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  275\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5040727, 0.5044203)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  276\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5044203, 0.50476533)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  277\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50476533, 0.5043885)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  278\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5043885, 0.5044444)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  279\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5044444, 0.50435716)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  280\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50435716, 0.50427043)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  281\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50427043, 0.5038298)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  282\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5038298, 0.50360423)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  283\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50360423, 0.503169)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  284\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.503169, 0.5029474)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  285\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5029474, 0.5032168)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  286\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.5032168, 0.5032056)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  287\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5032056, 0.5034722)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  288\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5034722, 0.5035294)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  289\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5035294, 0.50310344)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  290\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50310344, 0.5028866)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  291\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5028866, 0.5030137)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  292\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5030137, 0.5028669)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  293\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5028669, 0.5027211)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  294\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5027211, 0.50277966)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  295\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50277966, 0.5029054)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  296\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5029054, 0.50276095)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  297\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50276095, 0.5026846)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  298\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5026846, 0.5022743)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  299\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5022743, 0.50226665)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  300\n",
            "Loss 2.5700573921203613\n",
            "accuracy:  (0.50226665, 0.50225914)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  301\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50225914, 0.5023841)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  302\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5023841, 0.50237626)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  303\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50237626, 0.50269735)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  304\n",
            "Loss 0.7971624135971069\n",
            "accuracy:  (0.50269735, 0.50236064)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  305\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50236064, 0.5026144)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  306\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5026144, 0.5025407)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  307\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5025407, 0.5024675)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  308\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5024675, 0.50213593)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  309\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50213593, 0.50174195)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  310\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50174195, 0.5019293)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  311\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5019293, 0.5021795)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  312\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5021795, 0.50210863)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  313\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50210863, 0.50165606)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  314\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50165606, 0.50184125)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  315\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50184125, 0.50170887)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  316\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50170887, 0.50157726)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  317\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50157726, 0.50144655)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  318\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50144655, 0.501442)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  319\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.501442, 0.501375)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  320\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.501375, 0.5013084)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  321\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5013084, 0.5011801)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  322\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011801, 0.50111455)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  323\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50111455, 0.5011111)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  324\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011111, 0.50073844)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  325\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50073844, 0.5003067)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  326\n",
            "Loss 1.9040805101394653\n",
            "accuracy:  (0.5003067, 0.49975535)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  327\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49975535, 0.49969512)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  328\n",
            "Loss 2.508453607559204\n",
            "accuracy:  (0.49969512, 0.49927053)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  329\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49927053, 0.49951515)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  330\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49951515, 0.4995166)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  331\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4995166, 0.49957833)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  332\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49957833, 0.49963963)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  333\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49963963, 0.4997006)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  334\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4997006, 0.4997612)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  335\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4997612, 0.4997024)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  336\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4997024, 0.5001187)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  337\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5001187, 0.5)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  338\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.5, 0.500118)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  339\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.500118, 0.5007059)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  340\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5007059, 0.5003519)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  341\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.5003519, 0.5005263)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  342\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5005263, 0.50052476)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  343\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50052476, 0.5004651)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  344\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5004651, 0.5005797)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  345\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5005797, 0.50063586)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  346\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50063586, 0.5009222)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  347\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009222, 0.5012069)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  348\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012069, 0.50143266)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  349\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50143266, 0.5012571)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  350\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012571, 0.50153846)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  351\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50153846, 0.5016477)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  352\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016477, 0.50164306)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  353\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50164306, 0.5016384)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  354\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016384, 0.50169015)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  355\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50169015, 0.5013483)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  356\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5013483, 0.5010644)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  357\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010644, 0.5013408)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  358\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5013408, 0.50133705)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  359\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50133705, 0.5012778)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  360\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012778, 0.50138503)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  361\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50138503, 0.5011602)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  362\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011602, 0.5011019)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  363\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011019, 0.5011538)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  364\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011538, 0.50115067)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  365\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50115067, 0.50103825)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  366\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50103825, 0.50103545)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  367\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50103545, 0.50125)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  368\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50125, 0.5013008)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  369\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5013008, 0.5010811)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  370\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010811, 0.50102425)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  371\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50102425, 0.5010215)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  372\n",
            "Loss 0.6654213070869446\n",
            "accuracy:  (0.5010215, 0.50075066)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  373\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50075066, 0.5009091)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  374\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.5009091, 0.50090665)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  375\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50090665, 0.5010638)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  376\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010638, 0.501061)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  377\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.501061, 0.5013757)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  378\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5013757, 0.50153035)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  379\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50153035, 0.50163156)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  380\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50163156, 0.5015748)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  381\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5015748, 0.50172776)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  382\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50172776, 0.50146216)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  383\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50146216, 0.50130206)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  384\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50130206, 0.5012987)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  385\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012987, 0.50124353)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  386\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50124353, 0.5012403)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  387\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012403, 0.50097936)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  388\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50097936, 0.5009254)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  389\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009254, 0.50051284)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  390\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50051284, 0.50056267)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  391\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50056267, 0.50056124)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  392\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50056124, 0.500458)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  393\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.500458, 0.5002538)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  394\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5002538, 0.50025314)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  395\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50025314, 0.50010103)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  396\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50010103, 0.49964735)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  397\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49964735, 0.4996985)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  398\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.4996985, 0.49994987)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  399\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49994987, 0.49995)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  400\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.49995, 0.5002494)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  401\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5002494, 0.5005473)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  402\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5005473, 0.50064516)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  403\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.50064516, 0.50074255)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  404\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50074255, 0.50083953)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  405\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50083953, 0.5008867)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  406\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5008867, 0.50117934)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  407\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50117934, 0.5010784)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  408\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010784, 0.5017115)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  409\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5017115, 0.50141466)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  410\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50141466, 0.50150853)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  411\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50150853, 0.50150484)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  412\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50150484, 0.5017433)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  413\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5017433, 0.50173914)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  414\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50173914, 0.5015904)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  415\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5015904, 0.5016346)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  416\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016346, 0.50182253)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  417\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50182253, 0.5016268)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  418\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016268, 0.50167066)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  419\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50167066, 0.50185716)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  420\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50185716, 0.50194776)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  421\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50194776, 0.5016588)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  422\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016588, 0.501844)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  423\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.501844, 0.50183964)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  424\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50183964, 0.5015059)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  425\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5015059, 0.5016432)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  426\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016432, 0.50177985)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  427\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50177985, 0.5016822)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  428\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016822, 0.5016317)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  429\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016317, 0.5017209)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  430\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5017209, 0.5015313)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  431\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5015313, 0.50175923)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  432\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50175923, 0.501709)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  433\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.501709, 0.5016129)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  434\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5016129, 0.5015632)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  435\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5015632, 0.5014679)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  436\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5014679, 0.5011442)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  437\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011442, 0.50105023)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  438\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50105023, 0.5009112)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  439\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009112, 0.5007727)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  440\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5007727, 0.5008617)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  441\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5008617, 0.50072396)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  442\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50072396, 0.5005869)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  443\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5005869, 0.5009459)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  444\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009459, 0.5008989)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  445\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5008989, 0.50071746)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  446\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50071746, 0.5007159)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  447\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5007159, 0.50084823)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  448\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50084823, 0.5008018)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  449\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5008018, 0.5008889)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  450\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5008889, 0.50070953)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  451\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50070953, 0.50088495)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  452\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50088495, 0.5009713)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  453\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009713, 0.50105727)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  454\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50105727, 0.5012747)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  455\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012747, 0.5011842)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  456\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011842, 0.5011816)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  457\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011816, 0.5012227)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  458\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012227, 0.50165576)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  459\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50165576, 0.50147825)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  460\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50147825, 0.5015184)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  461\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5015184, 0.5014719)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  462\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5014719, 0.5013391)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  463\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5013391, 0.5012069)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  464\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012069, 0.50103223)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  465\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50103223, 0.50107294)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  466\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50107294, 0.5009422)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  467\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009422, 0.5011538)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  468\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011538, 0.50110877)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  469\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50110877, 0.5009787)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  470\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009787, 0.5009766)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  471\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009766, 0.50114405)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  472\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50114405, 0.5012262)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  473\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5012262, 0.5010548)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  474\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010548, 0.5010526)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  475\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010526, 0.5011765)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  476\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011765, 0.5011321)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  477\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011321, 0.50129706)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  478\n",
            "Loss 0.6792842745780945\n",
            "accuracy:  (0.50129706, 0.5011691)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  479\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011691, 0.5010833)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  480\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010833, 0.5009979)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  481\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009979, 0.50107884)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  482\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50107884, 0.5010766)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  483\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010766, 0.50099176)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  484\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50099176, 0.5011134)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  485\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011134, 0.5011934)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  486\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5011934, 0.5009856)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  487\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009856, 0.50086063)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  488\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50086063, 0.5010225)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  489\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5010225, 0.50081635)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  490\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50081635, 0.5007332)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  491\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5007332, 0.50085366)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  492\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50085366, 0.5007302)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  493\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5007302, 0.5008907)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  494\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5008907, 0.5007273)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  495\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5007273, 0.50080645)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  496\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50080645, 0.50092554)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  497\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.50092554, 0.5009237)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  498\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5009237, 0.5007615)\n",
            "(50, 300)\n",
            "(50,)\n",
            "For iter  499\n",
            "Loss 0.6931471824645996\n",
            "accuracy:  (0.5007615, 0.50072)\n",
            "/content/gdrive/My Drive/model/modelx1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sZY_znEovOAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "808d7853-a27c-42b2-e4c1-ffc6c0778f7e"
      },
      "cell_type": "code",
      "source": [
        "word = model['widespread']\n",
        "modelx.predict(vec_title)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/model/modelx1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "metadata": {
        "id": "_XsX1yOMzlNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  title = dismissed['Title paper'][0][1:(len(dismissed['Title paper'][9])-1)].split(' ')\n",
        "  \n",
        "  #300 is the shape of the vector representation of a word in the google word2vec model\n",
        "  vec_title = np.zeros((300))\n",
        "  count = 0\n",
        "  for j in range(len(title)):\n",
        "    word = title[j]\n",
        "    if (word in model): \n",
        "      vec_title += model[title[j]]\n",
        "      count += 1\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPDNT69z5L7M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3023
        },
        "outputId": "2652c9fe-6e64-4308-a27e-d79d78f76ca7"
      },
      "cell_type": "code",
      "source": [
        "dismissed"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Author</th>\n",
              "      <th>Title paper</th>\n",
              "      <th>Journal</th>\n",
              "      <th>Year</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>AU=AYDIN AYDIN</td>\n",
              "      <td>[Widespread involvement of hepatic, renal and ...</td>\n",
              "      <td>[TURKISH JOURNAL OF PEDIATRICS]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[mycotic aneurysm, visceral arteries, mesenter...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>AU=AYDIN AYDIN</td>\n",
              "      <td>[Temporal Analysis of Finger-Tapping Test in I...</td>\n",
              "      <td>[ACTA PHYSIOLOGICA]</td>\n",
              "      <td>[2015]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>AU=AYDIN AYDIN</td>\n",
              "      <td>[UNDIFFERENTIATED CARCINOMA OF THE EPIDIDYMIS]</td>\n",
              "      <td>[ACTA CLINICA CROATICA]</td>\n",
              "      <td>[2011]</td>\n",
              "      <td>[Epididymis, Undiffrentiated carcinomas, Metas...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>AU=AYDIN AYDIN</td>\n",
              "      <td>[Undifferentiated Primary Carcinoma of the Epi...</td>\n",
              "      <td>[UHOD-ULUSLARARASI HEMATOLOJI-ONKOLOJI DERGISI]</td>\n",
              "      <td>[2009]</td>\n",
              "      <td>[Epididymis, Undifferentiated carcinoma, Adjuv...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>AU=GİLGİL ERDAL</td>\n",
              "      <td>[Internal iliac artery pseudoaneurysm - An unu...</td>\n",
              "      <td>[AMERICAN JOURNAL OF PHYSICAL MEDICINE &amp; REHAB...</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>[lumbosacral plexopathy, sciatica, pseudoaneur...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>AU=GİLGİL ERDAL</td>\n",
              "      <td>[Bone metabolism and mineral density in patien...</td>\n",
              "      <td>[SAUDI MEDICAL JOURNAL]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>AU=GİLGİL ERDAL</td>\n",
              "      <td>[Reply to the report: epidemiology of rheumato...</td>\n",
              "      <td>[CLINICAL RHEUMATOLOGY]</td>\n",
              "      <td>[2006]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>AU=GİLGİL ERDAL</td>\n",
              "      <td>[Prevalence of Rheumatoid Arthritis and Spondy...</td>\n",
              "      <td>[ARCHIVES OF RHEUMATOLOGY]</td>\n",
              "      <td>[2018]</td>\n",
              "      <td>[Epidemiology, prevalence, rheumatoid arthriti...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>AU=GİLGİL ERDAL</td>\n",
              "      <td>[Medial collateral ligament bursitis in a pati...</td>\n",
              "      <td>[JOURNAL OF BACK AND MUSCULOSKELETAL REHABILIT...</td>\n",
              "      <td>[2018]</td>\n",
              "      <td>[Medial collateral ligament (MCL), bursitis, k...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>AU=HALAÇ METİN</td>\n",
              "      <td>[Diagnostic value of magnetic resonance imagin...</td>\n",
              "      <td>[MEDICAL ONCOLOGY]</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>[Breast cancer, MRI, Bone scan, Bone metastasis]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>AU=HALAÇ METİN</td>\n",
              "      <td>[Situs inversus totalis shown in the 18F-FDG P...</td>\n",
              "      <td>[HELLENIC JOURNAL OF NUCLEAR MEDICINE]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>AU=HALAÇ METİN</td>\n",
              "      <td>[Fluoro-18 fluorodeoxyglucose positron emissio...</td>\n",
              "      <td>[HELLENIC JOURNAL OF NUCLEAR MEDICINE]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[F-18-FDG, PET imaging, penile carcinoma]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>AU=HALAÇ METİN</td>\n",
              "      <td>[FDG PET/CT findings in primary hyperparathyro...</td>\n",
              "      <td>[EUROPEAN JOURNAL OF NUCLEAR MEDICINE AND MOLE...</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>AU=HALAÇ METİN</td>\n",
              "      <td>[PET/CT findings in a multicentric form of Cas...</td>\n",
              "      <td>[HELLENIC JOURNAL OF NUCLEAR MEDICINE]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[Castleman's disease, F-18-FDG PET/CT, differe...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>AU=KIRIŞ ABDULKADİR</td>\n",
              "      <td>[Relationship between arterial stiffness and m...</td>\n",
              "      <td>[AMERICAN JOURNAL OF HYPERTENSION]</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>AU=KIRIŞ ABDULKADİR</td>\n",
              "      <td>[Assessment of left ventricular diastolic func...</td>\n",
              "      <td>[ECHOCARDIOGRAPHY-A JOURNAL OF CARDIOVASCULAR ...</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>[Cushing's Syndrome, left ventricular function...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>AU=KIRIŞ ABDULKADİR</td>\n",
              "      <td>[Immediate procedural and long-term clinical o...</td>\n",
              "      <td>[AMERICAN JOURNAL OF CARDIOLOGY]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>AU=KIRIŞ ABDULKADİR</td>\n",
              "      <td>[Relationship between aortic stiffness and the...</td>\n",
              "      <td>[ANGIOLOGY]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>AU=KIRIŞ ABDULKADİR</td>\n",
              "      <td>[Diagonal ear-lobe crease is associated with c...</td>\n",
              "      <td>[ATHEROSCLEROSIS]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[carotid artery intima-media thickness, diagon...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>AU=KÜÇÜKBAYRAK ABDULKADİR</td>\n",
              "      <td>[Multiple brain abscesses and mastoiditis due ...</td>\n",
              "      <td>[NEUROSURGERY QUARTERLY]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[Morganella morganii, subdural abscesses, pare...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>AU=KÜÇÜKBAYRAK ABDULKADİR</td>\n",
              "      <td>[Hand carriage of Candida species and risk fac...</td>\n",
              "      <td>[MYCOSES]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[Candida carriage, hospital personnel, risk fa...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>AU=KÜÇÜKBAYRAK ABDULKADİR</td>\n",
              "      <td>[Comparison of classical methods versus BACTEC...</td>\n",
              "      <td>[RUSSIAN OPEN MEDICAL JOURNAL]</td>\n",
              "      <td>[2015]</td>\n",
              "      <td>[sterile body fluids, BACTEC, blood culture bo...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>AU=KÜÇÜKBAYRAK ABDULKADİR</td>\n",
              "      <td>[Impact of antimicrobial drug restrictions on ...</td>\n",
              "      <td>[TURKISH JOURNAL OF MEDICAL SCIENCES]</td>\n",
              "      <td>[2016]</td>\n",
              "      <td>[Antibiotic policy, antibiotic restriction, an...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>AU=KÜÇÜKBAYRAK ABDULKADİR</td>\n",
              "      <td>[ASCARIS LUMBRICOIDES PRESENTING AS AN OMENTAL...</td>\n",
              "      <td>[ACTA MEDICA MEDITERRANEA]</td>\n",
              "      <td>[2014]</td>\n",
              "      <td>[Ascariasis, Omentum, Pregnancy]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>AU=YILDIRIM ABDULKADİR</td>\n",
              "      <td>[Spirulina platensis Protects against Gentamic...</td>\n",
              "      <td>[PHYTOTHERAPY RESEARCH]</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>[Spirulina platensis, gentamicin sulphate, nep...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>AU=YILDIRIM ABDULKADİR</td>\n",
              "      <td>[Protective effects of N-acetylcysteine and Gi...</td>\n",
              "      <td>[CLINICAL AND EXPERIMENTAL MEDICINE]</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>[Ischaemia-reperfusion, DNA damage, 8-Hydroxyd...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>AU=YILDIRIM ABDULKADİR</td>\n",
              "      <td>[Increased lipid peroxidation and decreased an...</td>\n",
              "      <td>[TURKISH JOURNAL OF MEDICAL SCIENCES]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[ischemic stroke, oxidative stress, cerebrospi...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>AU=YILDIRIM ABDULKADİR</td>\n",
              "      <td>[Total antioxidant capacity and antioxidant en...</td>\n",
              "      <td>[JOURNAL OF PERIODONTOLOGY]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[antioxidant, glutathione peroxidase, malondia...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>AU=YILDIRIM ABDULKADİR</td>\n",
              "      <td>[Periodontal disease increases the risk of sev...</td>\n",
              "      <td>[JOURNAL OF CLINICAL PERIODONTOLOGY]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[mild pre-eclampsia, periodontal diseases, pro...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>AU=ŞENGÜN ABDULKADİR</td>\n",
              "      <td>[Effect of a new restoration technique on frac...</td>\n",
              "      <td>[DENTAL TRAUMATOLOGY]</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12271</th>\n",
              "      <td>12271</td>\n",
              "      <td>AU=ARSLAN ŞÜKRÜ</td>\n",
              "      <td>[Effect of chronic toluene exposure on heart r...</td>\n",
              "      <td>[PACE-PACING AND CLINICAL ELECTROPHYSIOLOGY]</td>\n",
              "      <td>[2018]</td>\n",
              "      <td>[arrhythmia, heart rate variability, toluene e...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12272</th>\n",
              "      <td>12272</td>\n",
              "      <td>AU=ARSLAN ŞÜKRÜ</td>\n",
              "      <td>[Demographic and clinical characteristics of p...</td>\n",
              "      <td>[CLINICAL RHEUMATOLOGY]</td>\n",
              "      <td>[2018]</td>\n",
              "      <td>[Children, Polyarthralgia, Rash, Serum sicknes...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12273</th>\n",
              "      <td>12273</td>\n",
              "      <td>AU=DÜZGÜN ŞÜKRÜ</td>\n",
              "      <td>[Objectively structured verbal examination to ...</td>\n",
              "      <td>[TURKISH JOURNAL OF SURGERY]</td>\n",
              "      <td>[2018]</td>\n",
              "      <td>[Clinical reasoning, exam, Medical education]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12274</th>\n",
              "      <td>12274</td>\n",
              "      <td>AU=DÜZGÜN ŞÜKRÜ</td>\n",
              "      <td>[The neutrophil-to-lymphocyte ratio as a diagn...</td>\n",
              "      <td>[TURKISH JOURNAL OF BIOCHEMISTRY-TURK BIYOKIMY...</td>\n",
              "      <td>[2017]</td>\n",
              "      <td>[Thyroid, Benign, Malignant, Neutrophil, Lymph...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12275</th>\n",
              "      <td>12275</td>\n",
              "      <td>AU=DÜZGÜN ŞÜKRÜ</td>\n",
              "      <td>[Differential effects of p38 MAP kinase inhibi...</td>\n",
              "      <td>[CYTOTECHNOLOGY]</td>\n",
              "      <td>[2017]</td>\n",
              "      <td>[Cancer, p38 MAPK, Proliferation, Metastasis]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12276</th>\n",
              "      <td>12276</td>\n",
              "      <td>AU=DÜZGÜN ŞÜKRÜ</td>\n",
              "      <td>[Reply to the Letter to the Editor: \"Crystalli...</td>\n",
              "      <td>[ASIAN JOURNAL OF SURGERY]</td>\n",
              "      <td>[2016]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12277</th>\n",
              "      <td>12277</td>\n",
              "      <td>AU=DÜZGÜN ŞÜKRÜ</td>\n",
              "      <td>[Prolidase activity and oxidative stress in pa...</td>\n",
              "      <td>[ANNALI ITALIANI DI CHIRURGIA]</td>\n",
              "      <td>[2016]</td>\n",
              "      <td>[Breast carcinoma, Oxidative stress, Proline d...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12278</th>\n",
              "      <td>12278</td>\n",
              "      <td>AU=BOYLU ŞÜKRÜ</td>\n",
              "      <td>[Brown tumors mimicking bone metastases]</td>\n",
              "      <td>[JOURNAL OF THE NATIONAL MEDICAL ASSOCIATION]</td>\n",
              "      <td>[2006]</td>\n",
              "      <td>[brown tumor, hyperparathyroidism, bone scinti...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12279</th>\n",
              "      <td>12279</td>\n",
              "      <td>AU=BOYLU ŞÜKRÜ</td>\n",
              "      <td>[Hyaluronic acid/carboxymethylcellulose membra...</td>\n",
              "      <td>[EUROPEAN SURGICAL RESEARCH]</td>\n",
              "      <td>[2006]</td>\n",
              "      <td>[polypropylene mesh, hyaluronic acid/carboxyme...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12280</th>\n",
              "      <td>12280</td>\n",
              "      <td>AU=BOYLU ŞÜKRÜ</td>\n",
              "      <td>[Contribution of an educational video to surgi...</td>\n",
              "      <td>[TURKISH JOURNAL OF SURGERY]</td>\n",
              "      <td>[2017]</td>\n",
              "      <td>[Education, laparoscopic surgery, video]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12281</th>\n",
              "      <td>12281</td>\n",
              "      <td>AU=BOYLU ŞÜKRÜ</td>\n",
              "      <td>[Factors affecting breast cancer treatment del...</td>\n",
              "      <td>[EUROPEAN JOURNAL OF PUBLIC HEALTH]</td>\n",
              "      <td>[2015]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12282</th>\n",
              "      <td>12282</td>\n",
              "      <td>AU=BOYLU ŞÜKRÜ</td>\n",
              "      <td>[Factors affecting time to seeking medical adv...</td>\n",
              "      <td>[JOURNAL OF CLINICAL ONCOLOGY]</td>\n",
              "      <td>[2013]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12283</th>\n",
              "      <td>12283</td>\n",
              "      <td>AU=KARAKAYA ŞÜKRÜ</td>\n",
              "      <td>[Simulation of lightning strike damage in carb...</td>\n",
              "      <td>[JOURNAL OF REINFORCED PLASTICS AND COMPOSITES]</td>\n",
              "      <td>[2016]</td>\n",
              "      <td>[CFRP composites, lightning damage, CNT]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12284</th>\n",
              "      <td>12284</td>\n",
              "      <td>AU=KARAKAYA ŞÜKRÜ</td>\n",
              "      <td>[Self-Deploying Composite Shell Reflector Ante...</td>\n",
              "      <td>[PROCEEDINGS OF 6TH INTERNATIONAL CONFERENCE O...</td>\n",
              "      <td>[2013]</td>\n",
              "      <td>[Composite reflector, Ku-band antenna, shell s...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12285</th>\n",
              "      <td>12285</td>\n",
              "      <td>AU=KARAKAYA ŞÜKRÜ</td>\n",
              "      <td>[INVESTIGATION OF HYBRID AND DIFFERENT CROSS-S...</td>\n",
              "      <td>[TRANSACTIONS OF THE CANADIAN SOCIETY FOR MECH...</td>\n",
              "      <td>[2012]</td>\n",
              "      <td>[disc spring, hybridization, FEM, carbon fiber]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12286</th>\n",
              "      <td>12286</td>\n",
              "      <td>AU=KARAKAYA ŞÜKRÜ</td>\n",
              "      <td>[Analysis and testing of ultrathin shell 2m di...</td>\n",
              "      <td>[JOURNAL OF REINFORCED PLASTICS AND COMPOSITES]</td>\n",
              "      <td>[2013]</td>\n",
              "      <td>[Deployable reflector, shell structure, carbon...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12287</th>\n",
              "      <td>12287</td>\n",
              "      <td>AU=KARAKAYA ŞÜKRÜ</td>\n",
              "      <td>[Manufacturing Of Thin-Shell Offset Demonstrat...</td>\n",
              "      <td>[MATERIALS AND MANUFACTURING TECHNOLOGIES XIV]</td>\n",
              "      <td>[2012]</td>\n",
              "      <td>[reflectors, composites, resin infusion, shells]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12288</th>\n",
              "      <td>12288</td>\n",
              "      <td>AU=TAKTAK ŞÜKRÜ</td>\n",
              "      <td>[High temperature wear and friction properties...</td>\n",
              "      <td>[SURFACE &amp; COATINGS TECHNOLOGY]</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>[TRD chromizing, plasma nitriding, duplex trea...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12289</th>\n",
              "      <td>12289</td>\n",
              "      <td>AU=TAKTAK ŞÜKRÜ</td>\n",
              "      <td>[Some mechanical properties of borided AISI H1...</td>\n",
              "      <td>[MATERIALS &amp; DESIGN]</td>\n",
              "      <td>[2007]</td>\n",
              "      <td>[AISI 304, AISI H13, salt bath boriding, adhes...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12290</th>\n",
              "      <td>12290</td>\n",
              "      <td>AU=TAKTAK ŞÜKRÜ</td>\n",
              "      <td>[Tribological behaviour of borided bearing ste...</td>\n",
              "      <td>[SURFACE &amp; COATINGS TECHNOLOGY]</td>\n",
              "      <td>[2006]</td>\n",
              "      <td>[borided 52100, borided 440C, diffusion coatin...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12291</th>\n",
              "      <td>12291</td>\n",
              "      <td>AU=TAKTAK ŞÜKRÜ</td>\n",
              "      <td>[A study on the diffusion kinetics of borides ...</td>\n",
              "      <td>[JOURNAL OF MATERIALS SCIENCE]</td>\n",
              "      <td>[2006]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12292</th>\n",
              "      <td>12292</td>\n",
              "      <td>AU=TAKTAK ŞÜKRÜ</td>\n",
              "      <td>[Identification of delamination failure of bor...</td>\n",
              "      <td>[JOURNAL OF MATERIALS ENGINEERING AND PERFORMA...</td>\n",
              "      <td>[2006]</td>\n",
              "      <td>[adhesion, boriding, delamination failure, fra...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12293</th>\n",
              "      <td>12293</td>\n",
              "      <td>AU=ÇAKMAKTEPE ŞÜKRÜ</td>\n",
              "      <td>[The energy spectrum of carriers between two c...</td>\n",
              "      <td>[JOURNAL OF NANOMATERIALS]</td>\n",
              "      <td>[2006]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12294</th>\n",
              "      <td>12294</td>\n",
              "      <td>AU=ÇAKMAKTEPE ŞÜKRÜ</td>\n",
              "      <td>[Deposition of (Ni80Fe20)(100-x)Cr-x Alloy Thi...</td>\n",
              "      <td>[IEEE TRANSACTIONS ON MAGNETICS]</td>\n",
              "      <td>[2016]</td>\n",
              "      <td>[Alloy thin films, dc magnetron sputtering, ma...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12295</th>\n",
              "      <td>12295</td>\n",
              "      <td>AU=ÇAKMAKTEPE ŞÜKRÜ</td>\n",
              "      <td>[A novel polyphenol-based ferromagnetic polyme...</td>\n",
              "      <td>[APPLIED PHYSICS A-MATERIALS SCIENCE &amp; PROCESS...</td>\n",
              "      <td>[2015]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12296</th>\n",
              "      <td>12296</td>\n",
              "      <td>AU=HİSAR ŞÜKRİYE</td>\n",
              "      <td>[Discrimination of penaeid shrimps with PCR-RF...</td>\n",
              "      <td>[JOURNAL OF SHELLFISH RESEARCH]</td>\n",
              "      <td>[2008]</td>\n",
              "      <td>[shrimp, PCR, RFLP, cytochrome-b]</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12297</th>\n",
              "      <td>12297</td>\n",
              "      <td>AU=HİSAR ŞÜKRİYE</td>\n",
              "      <td>[The effects of elevated carbon dioxide and te...</td>\n",
              "      <td>[ENVIRONMENTAL TOXICOLOGY AND PHARMACOLOGY]</td>\n",
              "      <td>[2016]</td>\n",
              "      <td>[Elevated carbon dioxide and temperature, Hema...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12298</th>\n",
              "      <td>12298</td>\n",
              "      <td>AU=HİSAR ŞÜKRİYE</td>\n",
              "      <td>[Melatonin Implantation in Preovulatory Rainbo...</td>\n",
              "      <td>[TURKISH JOURNAL OF FISHERIES AND AQUATIC SCIE...</td>\n",
              "      <td>[2014]</td>\n",
              "      <td>[Trout, melatonin implantation, short photoper...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12299</th>\n",
              "      <td>12299</td>\n",
              "      <td>AU=HİSAR ŞÜKRİYE</td>\n",
              "      <td>[The sexually dimorphic adipose fin is an andr...</td>\n",
              "      <td>[FISH PHYSIOLOGY AND BIOCHEMISTRY]</td>\n",
              "      <td>[2013]</td>\n",
              "      <td>[Adipose fin, Androgen receptor, Secondary sex...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12300</th>\n",
              "      <td>12300</td>\n",
              "      <td>AU=HİSAR ŞÜKRİYE</td>\n",
              "      <td>[Biogenic amines formation in Atlantic bonito ...</td>\n",
              "      <td>[EUROPEAN FOOD RESEARCH AND TECHNOLOGY]</td>\n",
              "      <td>[2011]</td>\n",
              "      <td>[Chitosan film, Atlantic Bonito, Modified atmo...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12301 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                     Author  \\\n",
              "0               0             AU=AYDIN AYDIN   \n",
              "1               1             AU=AYDIN AYDIN   \n",
              "2               2             AU=AYDIN AYDIN   \n",
              "3               3             AU=AYDIN AYDIN   \n",
              "4               4            AU=GİLGİL ERDAL   \n",
              "5               5            AU=GİLGİL ERDAL   \n",
              "6               6            AU=GİLGİL ERDAL   \n",
              "7               7            AU=GİLGİL ERDAL   \n",
              "8               8            AU=GİLGİL ERDAL   \n",
              "9               9             AU=HALAÇ METİN   \n",
              "10             10             AU=HALAÇ METİN   \n",
              "11             11             AU=HALAÇ METİN   \n",
              "12             12             AU=HALAÇ METİN   \n",
              "13             13             AU=HALAÇ METİN   \n",
              "14             14        AU=KIRIŞ ABDULKADİR   \n",
              "15             15        AU=KIRIŞ ABDULKADİR   \n",
              "16             16        AU=KIRIŞ ABDULKADİR   \n",
              "17             17        AU=KIRIŞ ABDULKADİR   \n",
              "18             18        AU=KIRIŞ ABDULKADİR   \n",
              "19             19  AU=KÜÇÜKBAYRAK ABDULKADİR   \n",
              "20             20  AU=KÜÇÜKBAYRAK ABDULKADİR   \n",
              "21             21  AU=KÜÇÜKBAYRAK ABDULKADİR   \n",
              "22             22  AU=KÜÇÜKBAYRAK ABDULKADİR   \n",
              "23             23  AU=KÜÇÜKBAYRAK ABDULKADİR   \n",
              "24             24     AU=YILDIRIM ABDULKADİR   \n",
              "25             25     AU=YILDIRIM ABDULKADİR   \n",
              "26             26     AU=YILDIRIM ABDULKADİR   \n",
              "27             27     AU=YILDIRIM ABDULKADİR   \n",
              "28             28     AU=YILDIRIM ABDULKADİR   \n",
              "29             29       AU=ŞENGÜN ABDULKADİR   \n",
              "...           ...                        ...   \n",
              "12271       12271            AU=ARSLAN ŞÜKRÜ   \n",
              "12272       12272            AU=ARSLAN ŞÜKRÜ   \n",
              "12273       12273            AU=DÜZGÜN ŞÜKRÜ   \n",
              "12274       12274            AU=DÜZGÜN ŞÜKRÜ   \n",
              "12275       12275            AU=DÜZGÜN ŞÜKRÜ   \n",
              "12276       12276            AU=DÜZGÜN ŞÜKRÜ   \n",
              "12277       12277            AU=DÜZGÜN ŞÜKRÜ   \n",
              "12278       12278             AU=BOYLU ŞÜKRÜ   \n",
              "12279       12279             AU=BOYLU ŞÜKRÜ   \n",
              "12280       12280             AU=BOYLU ŞÜKRÜ   \n",
              "12281       12281             AU=BOYLU ŞÜKRÜ   \n",
              "12282       12282             AU=BOYLU ŞÜKRÜ   \n",
              "12283       12283          AU=KARAKAYA ŞÜKRÜ   \n",
              "12284       12284          AU=KARAKAYA ŞÜKRÜ   \n",
              "12285       12285          AU=KARAKAYA ŞÜKRÜ   \n",
              "12286       12286          AU=KARAKAYA ŞÜKRÜ   \n",
              "12287       12287          AU=KARAKAYA ŞÜKRÜ   \n",
              "12288       12288            AU=TAKTAK ŞÜKRÜ   \n",
              "12289       12289            AU=TAKTAK ŞÜKRÜ   \n",
              "12290       12290            AU=TAKTAK ŞÜKRÜ   \n",
              "12291       12291            AU=TAKTAK ŞÜKRÜ   \n",
              "12292       12292            AU=TAKTAK ŞÜKRÜ   \n",
              "12293       12293        AU=ÇAKMAKTEPE ŞÜKRÜ   \n",
              "12294       12294        AU=ÇAKMAKTEPE ŞÜKRÜ   \n",
              "12295       12295        AU=ÇAKMAKTEPE ŞÜKRÜ   \n",
              "12296       12296           AU=HİSAR ŞÜKRİYE   \n",
              "12297       12297           AU=HİSAR ŞÜKRİYE   \n",
              "12298       12298           AU=HİSAR ŞÜKRİYE   \n",
              "12299       12299           AU=HİSAR ŞÜKRİYE   \n",
              "12300       12300           AU=HİSAR ŞÜKRİYE   \n",
              "\n",
              "                                             Title paper  \\\n",
              "0      [Widespread involvement of hepatic, renal and ...   \n",
              "1      [Temporal Analysis of Finger-Tapping Test in I...   \n",
              "2         [UNDIFFERENTIATED CARCINOMA OF THE EPIDIDYMIS]   \n",
              "3      [Undifferentiated Primary Carcinoma of the Epi...   \n",
              "4      [Internal iliac artery pseudoaneurysm - An unu...   \n",
              "5      [Bone metabolism and mineral density in patien...   \n",
              "6      [Reply to the report: epidemiology of rheumato...   \n",
              "7      [Prevalence of Rheumatoid Arthritis and Spondy...   \n",
              "8      [Medial collateral ligament bursitis in a pati...   \n",
              "9      [Diagnostic value of magnetic resonance imagin...   \n",
              "10     [Situs inversus totalis shown in the 18F-FDG P...   \n",
              "11     [Fluoro-18 fluorodeoxyglucose positron emissio...   \n",
              "12     [FDG PET/CT findings in primary hyperparathyro...   \n",
              "13     [PET/CT findings in a multicentric form of Cas...   \n",
              "14     [Relationship between arterial stiffness and m...   \n",
              "15     [Assessment of left ventricular diastolic func...   \n",
              "16     [Immediate procedural and long-term clinical o...   \n",
              "17     [Relationship between aortic stiffness and the...   \n",
              "18     [Diagonal ear-lobe crease is associated with c...   \n",
              "19     [Multiple brain abscesses and mastoiditis due ...   \n",
              "20     [Hand carriage of Candida species and risk fac...   \n",
              "21     [Comparison of classical methods versus BACTEC...   \n",
              "22     [Impact of antimicrobial drug restrictions on ...   \n",
              "23     [ASCARIS LUMBRICOIDES PRESENTING AS AN OMENTAL...   \n",
              "24     [Spirulina platensis Protects against Gentamic...   \n",
              "25     [Protective effects of N-acetylcysteine and Gi...   \n",
              "26     [Increased lipid peroxidation and decreased an...   \n",
              "27     [Total antioxidant capacity and antioxidant en...   \n",
              "28     [Periodontal disease increases the risk of sev...   \n",
              "29     [Effect of a new restoration technique on frac...   \n",
              "...                                                  ...   \n",
              "12271  [Effect of chronic toluene exposure on heart r...   \n",
              "12272  [Demographic and clinical characteristics of p...   \n",
              "12273  [Objectively structured verbal examination to ...   \n",
              "12274  [The neutrophil-to-lymphocyte ratio as a diagn...   \n",
              "12275  [Differential effects of p38 MAP kinase inhibi...   \n",
              "12276  [Reply to the Letter to the Editor: \"Crystalli...   \n",
              "12277  [Prolidase activity and oxidative stress in pa...   \n",
              "12278           [Brown tumors mimicking bone metastases]   \n",
              "12279  [Hyaluronic acid/carboxymethylcellulose membra...   \n",
              "12280  [Contribution of an educational video to surgi...   \n",
              "12281  [Factors affecting breast cancer treatment del...   \n",
              "12282  [Factors affecting time to seeking medical adv...   \n",
              "12283  [Simulation of lightning strike damage in carb...   \n",
              "12284  [Self-Deploying Composite Shell Reflector Ante...   \n",
              "12285  [INVESTIGATION OF HYBRID AND DIFFERENT CROSS-S...   \n",
              "12286  [Analysis and testing of ultrathin shell 2m di...   \n",
              "12287  [Manufacturing Of Thin-Shell Offset Demonstrat...   \n",
              "12288  [High temperature wear and friction properties...   \n",
              "12289  [Some mechanical properties of borided AISI H1...   \n",
              "12290  [Tribological behaviour of borided bearing ste...   \n",
              "12291  [A study on the diffusion kinetics of borides ...   \n",
              "12292  [Identification of delamination failure of bor...   \n",
              "12293  [The energy spectrum of carriers between two c...   \n",
              "12294  [Deposition of (Ni80Fe20)(100-x)Cr-x Alloy Thi...   \n",
              "12295  [A novel polyphenol-based ferromagnetic polyme...   \n",
              "12296  [Discrimination of penaeid shrimps with PCR-RF...   \n",
              "12297  [The effects of elevated carbon dioxide and te...   \n",
              "12298  [Melatonin Implantation in Preovulatory Rainbo...   \n",
              "12299  [The sexually dimorphic adipose fin is an andr...   \n",
              "12300  [Biogenic amines formation in Atlantic bonito ...   \n",
              "\n",
              "                                                 Journal    Year  \\\n",
              "0                        [TURKISH JOURNAL OF PEDIATRICS]  [2007]   \n",
              "1                                    [ACTA PHYSIOLOGICA]  [2015]   \n",
              "2                                [ACTA CLINICA CROATICA]  [2011]   \n",
              "3        [UHOD-ULUSLARARASI HEMATOLOJI-ONKOLOJI DERGISI]  [2009]   \n",
              "4      [AMERICAN JOURNAL OF PHYSICAL MEDICINE & REHAB...  [2008]   \n",
              "5                                [SAUDI MEDICAL JOURNAL]  [2007]   \n",
              "6                                [CLINICAL RHEUMATOLOGY]  [2006]   \n",
              "7                             [ARCHIVES OF RHEUMATOLOGY]  [2018]   \n",
              "8      [JOURNAL OF BACK AND MUSCULOSKELETAL REHABILIT...  [2018]   \n",
              "9                                     [MEDICAL ONCOLOGY]  [2008]   \n",
              "10                [HELLENIC JOURNAL OF NUCLEAR MEDICINE]  [2007]   \n",
              "11                [HELLENIC JOURNAL OF NUCLEAR MEDICINE]  [2007]   \n",
              "12     [EUROPEAN JOURNAL OF NUCLEAR MEDICINE AND MOLE...  [2008]   \n",
              "13                [HELLENIC JOURNAL OF NUCLEAR MEDICINE]  [2007]   \n",
              "14                    [AMERICAN JOURNAL OF HYPERTENSION]  [2008]   \n",
              "15     [ECHOCARDIOGRAPHY-A JOURNAL OF CARDIOVASCULAR ...  [2008]   \n",
              "16                      [AMERICAN JOURNAL OF CARDIOLOGY]  [2007]   \n",
              "17                                           [ANGIOLOGY]  [2007]   \n",
              "18                                     [ATHEROSCLEROSIS]  [2007]   \n",
              "19                              [NEUROSURGERY QUARTERLY]  [2007]   \n",
              "20                                             [MYCOSES]  [2007]   \n",
              "21                        [RUSSIAN OPEN MEDICAL JOURNAL]  [2015]   \n",
              "22                 [TURKISH JOURNAL OF MEDICAL SCIENCES]  [2016]   \n",
              "23                            [ACTA MEDICA MEDITERRANEA]  [2014]   \n",
              "24                               [PHYTOTHERAPY RESEARCH]  [2008]   \n",
              "25                  [CLINICAL AND EXPERIMENTAL MEDICINE]  [2008]   \n",
              "26                 [TURKISH JOURNAL OF MEDICAL SCIENCES]  [2007]   \n",
              "27                           [JOURNAL OF PERIODONTOLOGY]  [2007]   \n",
              "28                  [JOURNAL OF CLINICAL PERIODONTOLOGY]  [2007]   \n",
              "29                                 [DENTAL TRAUMATOLOGY]  [2008]   \n",
              "...                                                  ...     ...   \n",
              "12271       [PACE-PACING AND CLINICAL ELECTROPHYSIOLOGY]  [2018]   \n",
              "12272                            [CLINICAL RHEUMATOLOGY]  [2018]   \n",
              "12273                       [TURKISH JOURNAL OF SURGERY]  [2018]   \n",
              "12274  [TURKISH JOURNAL OF BIOCHEMISTRY-TURK BIYOKIMY...  [2017]   \n",
              "12275                                   [CYTOTECHNOLOGY]  [2017]   \n",
              "12276                         [ASIAN JOURNAL OF SURGERY]  [2016]   \n",
              "12277                     [ANNALI ITALIANI DI CHIRURGIA]  [2016]   \n",
              "12278      [JOURNAL OF THE NATIONAL MEDICAL ASSOCIATION]  [2006]   \n",
              "12279                       [EUROPEAN SURGICAL RESEARCH]  [2006]   \n",
              "12280                       [TURKISH JOURNAL OF SURGERY]  [2017]   \n",
              "12281                [EUROPEAN JOURNAL OF PUBLIC HEALTH]  [2015]   \n",
              "12282                     [JOURNAL OF CLINICAL ONCOLOGY]  [2013]   \n",
              "12283    [JOURNAL OF REINFORCED PLASTICS AND COMPOSITES]  [2016]   \n",
              "12284  [PROCEEDINGS OF 6TH INTERNATIONAL CONFERENCE O...  [2013]   \n",
              "12285  [TRANSACTIONS OF THE CANADIAN SOCIETY FOR MECH...  [2012]   \n",
              "12286    [JOURNAL OF REINFORCED PLASTICS AND COMPOSITES]  [2013]   \n",
              "12287     [MATERIALS AND MANUFACTURING TECHNOLOGIES XIV]  [2012]   \n",
              "12288                    [SURFACE & COATINGS TECHNOLOGY]  [2008]   \n",
              "12289                               [MATERIALS & DESIGN]  [2007]   \n",
              "12290                    [SURFACE & COATINGS TECHNOLOGY]  [2006]   \n",
              "12291                     [JOURNAL OF MATERIALS SCIENCE]  [2006]   \n",
              "12292  [JOURNAL OF MATERIALS ENGINEERING AND PERFORMA...  [2006]   \n",
              "12293                         [JOURNAL OF NANOMATERIALS]  [2006]   \n",
              "12294                   [IEEE TRANSACTIONS ON MAGNETICS]  [2016]   \n",
              "12295  [APPLIED PHYSICS A-MATERIALS SCIENCE & PROCESS...  [2015]   \n",
              "12296                    [JOURNAL OF SHELLFISH RESEARCH]  [2008]   \n",
              "12297        [ENVIRONMENTAL TOXICOLOGY AND PHARMACOLOGY]  [2016]   \n",
              "12298  [TURKISH JOURNAL OF FISHERIES AND AQUATIC SCIE...  [2014]   \n",
              "12299                 [FISH PHYSIOLOGY AND BIOCHEMISTRY]  [2013]   \n",
              "12300            [EUROPEAN FOOD RESEARCH AND TECHNOLOGY]  [2011]   \n",
              "\n",
              "                                                Keywords  labels  \n",
              "0      [mycotic aneurysm, visceral arteries, mesenter...     1.0  \n",
              "1                                                    NaN     1.0  \n",
              "2      [Epididymis, Undiffrentiated carcinomas, Metas...     1.0  \n",
              "3      [Epididymis, Undifferentiated carcinoma, Adjuv...     1.0  \n",
              "4      [lumbosacral plexopathy, sciatica, pseudoaneur...     1.0  \n",
              "5                                                    NaN     1.0  \n",
              "6                                                    NaN     1.0  \n",
              "7      [Epidemiology, prevalence, rheumatoid arthriti...     1.0  \n",
              "8      [Medial collateral ligament (MCL), bursitis, k...     1.0  \n",
              "9       [Breast cancer, MRI, Bone scan, Bone metastasis]     1.0  \n",
              "10                                                   NaN     1.0  \n",
              "11             [F-18-FDG, PET imaging, penile carcinoma]     1.0  \n",
              "12                                                   NaN     1.0  \n",
              "13     [Castleman's disease, F-18-FDG PET/CT, differe...     1.0  \n",
              "14                                                   NaN     1.0  \n",
              "15     [Cushing's Syndrome, left ventricular function...     1.0  \n",
              "16                                                   NaN     1.0  \n",
              "17                                                   NaN     1.0  \n",
              "18     [carotid artery intima-media thickness, diagon...     1.0  \n",
              "19     [Morganella morganii, subdural abscesses, pare...     1.0  \n",
              "20     [Candida carriage, hospital personnel, risk fa...     1.0  \n",
              "21     [sterile body fluids, BACTEC, blood culture bo...     1.0  \n",
              "22     [Antibiotic policy, antibiotic restriction, an...     1.0  \n",
              "23                      [Ascariasis, Omentum, Pregnancy]     1.0  \n",
              "24     [Spirulina platensis, gentamicin sulphate, nep...     1.0  \n",
              "25     [Ischaemia-reperfusion, DNA damage, 8-Hydroxyd...     1.0  \n",
              "26     [ischemic stroke, oxidative stress, cerebrospi...     1.0  \n",
              "27     [antioxidant, glutathione peroxidase, malondia...     1.0  \n",
              "28     [mild pre-eclampsia, periodontal diseases, pro...     1.0  \n",
              "29                                                   NaN     1.0  \n",
              "...                                                  ...     ...  \n",
              "12271  [arrhythmia, heart rate variability, toluene e...     1.0  \n",
              "12272  [Children, Polyarthralgia, Rash, Serum sicknes...     1.0  \n",
              "12273      [Clinical reasoning, exam, Medical education]     1.0  \n",
              "12274  [Thyroid, Benign, Malignant, Neutrophil, Lymph...     1.0  \n",
              "12275      [Cancer, p38 MAPK, Proliferation, Metastasis]     1.0  \n",
              "12276                                                NaN     1.0  \n",
              "12277  [Breast carcinoma, Oxidative stress, Proline d...     1.0  \n",
              "12278  [brown tumor, hyperparathyroidism, bone scinti...     1.0  \n",
              "12279  [polypropylene mesh, hyaluronic acid/carboxyme...     1.0  \n",
              "12280           [Education, laparoscopic surgery, video]     1.0  \n",
              "12281                                                NaN     1.0  \n",
              "12282                                                NaN     1.0  \n",
              "12283           [CFRP composites, lightning damage, CNT]     1.0  \n",
              "12284  [Composite reflector, Ku-band antenna, shell s...     1.0  \n",
              "12285    [disc spring, hybridization, FEM, carbon fiber]     1.0  \n",
              "12286  [Deployable reflector, shell structure, carbon...     1.0  \n",
              "12287   [reflectors, composites, resin infusion, shells]     1.0  \n",
              "12288  [TRD chromizing, plasma nitriding, duplex trea...     1.0  \n",
              "12289  [AISI 304, AISI H13, salt bath boriding, adhes...     1.0  \n",
              "12290  [borided 52100, borided 440C, diffusion coatin...     1.0  \n",
              "12291                                                NaN     1.0  \n",
              "12292  [adhesion, boriding, delamination failure, fra...     1.0  \n",
              "12293                                                NaN     1.0  \n",
              "12294  [Alloy thin films, dc magnetron sputtering, ma...     1.0  \n",
              "12295                                                NaN     1.0  \n",
              "12296                  [shrimp, PCR, RFLP, cytochrome-b]     1.0  \n",
              "12297  [Elevated carbon dioxide and temperature, Hema...     1.0  \n",
              "12298  [Trout, melatonin implantation, short photoper...     1.0  \n",
              "12299  [Adipose fin, Androgen receptor, Secondary sex...     1.0  \n",
              "12300  [Chitosan film, Atlantic Bonito, Modified atmo...     1.0  \n",
              "\n",
              "[12301 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "metadata": {
        "id": "ov5zjshjgFcd",
        "colab_type": "code",
        "outputId": "c2288654-5d5d-4247-b288-c05b1670382c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "cell_type": "code",
      "source": [
        "#define constants[0:train_N]\n",
        "time_steps=10\n",
        "#hidden LSTM units\n",
        "num_units= 64*2\n",
        "\n",
        "#learning rate for adam\n",
        "learning_rate=0.001\n",
        "\n",
        "#size of batch\n",
        "batch_size=128\n",
        "\n",
        "iterations = 2000\n",
        "\n",
        "train_N = 90 #number of scenes used for training\n",
        "test_N = 100 - train_N\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "\n",
        "class Data():\n",
        "   def __init__(self, map_data_split, train = True):\n",
        "\n",
        "\n",
        "      self.data = np.array(data)\n",
        "      self.data_copy= list(data)\n",
        "\n",
        "      self.indexes = np.array(indexes)\n",
        "      self.means = np.array(means)\n",
        "      self.stds = np.array(stds)\n",
        "\n",
        "#For now, both train and test sets include all the dataset.\n",
        "trainSet = Data(map_data_split[0:train_N])\n",
        "testSet = Data(map_data_split[train_N:], train = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/gdrive/GoogleNews-vectors-negative300.bin.gz': No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-045792a20837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "UI1m3rD332Xd",
        "colab_type": "code",
        "outputId": "8f01c657-3ff4-4d96-c86f-cee2cfab4661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install scklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scklearn\n",
            "\u001b[31m  Could not find a version that satisfies the requirement scklearn (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for scklearn\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}