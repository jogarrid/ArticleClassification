{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import mpl_toolkits.mplot3d as mplt3d\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# shouldn't be enabled when using interactive 3D plots\n",
    "# %pylab inline\n",
    "# pylab.rcParams['figure.figsize'] = (10, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning & preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming chosen for same-length, to look pretty\n",
    "kicked = pd.read_csv('data/dismissed_complete.csv')\n",
    "stayed = pd.read_csv('data/nodismissed_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title paper</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Year</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AU=AYDIN AYDIN</td>\n",
       "      <td>[Widespread involvement of hepatic, renal and ...</td>\n",
       "      <td>[TURKISH JOURNAL OF PEDIATRICS]</td>\n",
       "      <td>[2007]</td>\n",
       "      <td>[mycotic aneurysm, visceral arteries, mesenter...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AU=AYDIN AYDIN</td>\n",
       "      <td>[Temporal Analysis of Finger-Tapping Test in I...</td>\n",
       "      <td>[ACTA PHYSIOLOGICA]</td>\n",
       "      <td>[2015]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AU=AYDIN AYDIN</td>\n",
       "      <td>[UNDIFFERENTIATED CARCINOMA OF THE EPIDIDYMIS]</td>\n",
       "      <td>[ACTA CLINICA CROATICA]</td>\n",
       "      <td>[2011]</td>\n",
       "      <td>[Epididymis, Undiffrentiated carcinomas, Metas...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AU=AYDIN AYDIN</td>\n",
       "      <td>[Undifferentiated Primary Carcinoma of the Epi...</td>\n",
       "      <td>[UHOD-ULUSLARARASI HEMATOLOJI-ONKOLOJI DERGISI]</td>\n",
       "      <td>[2009]</td>\n",
       "      <td>[Epididymis, Undifferentiated carcinoma, Adjuv...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AU=GİLGİL ERDAL</td>\n",
       "      <td>[Internal iliac artery pseudoaneurysm - An unu...</td>\n",
       "      <td>[AMERICAN JOURNAL OF PHYSICAL MEDICINE &amp; REHAB...</td>\n",
       "      <td>[2008]</td>\n",
       "      <td>[lumbosacral plexopathy, sciatica, pseudoaneur...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           Author  \\\n",
       "0           0   AU=AYDIN AYDIN   \n",
       "1           1   AU=AYDIN AYDIN   \n",
       "2           2   AU=AYDIN AYDIN   \n",
       "3           3   AU=AYDIN AYDIN   \n",
       "4           4  AU=GİLGİL ERDAL   \n",
       "\n",
       "                                         Title paper  \\\n",
       "0  [Widespread involvement of hepatic, renal and ...   \n",
       "1  [Temporal Analysis of Finger-Tapping Test in I...   \n",
       "2     [UNDIFFERENTIATED CARCINOMA OF THE EPIDIDYMIS]   \n",
       "3  [Undifferentiated Primary Carcinoma of the Epi...   \n",
       "4  [Internal iliac artery pseudoaneurysm - An unu...   \n",
       "\n",
       "                                             Journal    Year  \\\n",
       "0                    [TURKISH JOURNAL OF PEDIATRICS]  [2007]   \n",
       "1                                [ACTA PHYSIOLOGICA]  [2015]   \n",
       "2                            [ACTA CLINICA CROATICA]  [2011]   \n",
       "3    [UHOD-ULUSLARARASI HEMATOLOJI-ONKOLOJI DERGISI]  [2009]   \n",
       "4  [AMERICAN JOURNAL OF PHYSICAL MEDICINE & REHAB...  [2008]   \n",
       "\n",
       "                                            Keywords  labels  \n",
       "0  [mycotic aneurysm, visceral arteries, mesenter...     1.0  \n",
       "1                                                NaN     1.0  \n",
       "2  [Epididymis, Undiffrentiated carcinomas, Metas...     1.0  \n",
       "3  [Epididymis, Undifferentiated carcinoma, Adjuv...     1.0  \n",
       "4  [lumbosacral plexopathy, sciatica, pseudoaneur...     1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kicked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title paper</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Year</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AU=AU</td>\n",
       "      <td>[ASYMMETRIC LOADING OF AN EXTERNALLY CRACKED E...</td>\n",
       "      <td>[THEORETICAL AND APPLIED FRACTURE MECHANICS]</td>\n",
       "      <td>[1990]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AU=AU</td>\n",
       "      <td>[HIGH-CAPACITY OIL-WATER SEPARATOR PERFORMANCE...</td>\n",
       "      <td>[ENVIRONMENTAL ENGINEERING : PROCEEDINGS OF TH...</td>\n",
       "      <td>[1990]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AU=AU</td>\n",
       "      <td>[NUMERICAL MODELING OF BRIDGE FLAWS IN FIBER R...</td>\n",
       "      <td>[LOCALIZED DAMAGE COMPUTER-AIDED ASSESSMENT AN...</td>\n",
       "      <td>[1990]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AU=AU</td>\n",
       "      <td>[INHIBIN - A MARKER OF SERTOLI-CELL FUNCTION]</td>\n",
       "      <td>[HUMAN REPRODUCTION /]</td>\n",
       "      <td>[1988]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AU=AU</td>\n",
       "      <td>[THE MEASUREMENT OF BULK AND SURFACE RECOMBINA...</td>\n",
       "      <td>[CONFERENCE RECORD OF THE TWENTIETH IEEE PHOTO...</td>\n",
       "      <td>[1988]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Author                                        Title paper  \\\n",
       "0           0  AU=AU  [ASYMMETRIC LOADING OF AN EXTERNALLY CRACKED E...   \n",
       "1           1  AU=AU  [HIGH-CAPACITY OIL-WATER SEPARATOR PERFORMANCE...   \n",
       "2           2  AU=AU  [NUMERICAL MODELING OF BRIDGE FLAWS IN FIBER R...   \n",
       "3           3  AU=AU      [INHIBIN - A MARKER OF SERTOLI-CELL FUNCTION]   \n",
       "4           4  AU=AU  [THE MEASUREMENT OF BULK AND SURFACE RECOMBINA...   \n",
       "\n",
       "                                             Journal    Year Keywords  labels  \n",
       "0       [THEORETICAL AND APPLIED FRACTURE MECHANICS]  [1990]      NaN     0.0  \n",
       "1  [ENVIRONMENTAL ENGINEERING : PROCEEDINGS OF TH...  [1990]      NaN     0.0  \n",
       "2  [LOCALIZED DAMAGE COMPUTER-AIDED ASSESSMENT AN...  [1990]      NaN     0.0  \n",
       "3                             [HUMAN REPRODUCTION /]  [1988]      NaN     0.0  \n",
       "4  [CONFERENCE RECORD OF THE TWENTIETH IEEE PHOTO...  [1988]      NaN     0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stayed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12301, 7), (24859, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kicked.shape, stayed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just an experiment to try to learn on journal names, not on article names\n",
    "# kicked['Title paper'] = kicked['Journal']\n",
    "# stayed['Title paper'] = stayed['Journal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kicked1 = kicked[['Author','Title paper', 'labels']]\n",
    "stayed1 = stayed[['Author','Title paper', 'labels']]\n",
    "stayed1 = stayed1.sample(frac=(1.0 * kicked.shape[0])/stayed.shape[0]) # random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12301, 3), (12301, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make arrays equal in size\n",
    "kicked1.shape, stayed1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check some basic invariants on the input data, all should return True\n",
    "print(kicked1['Author'].apply(lambda x: x[:3] == 'AU=').all())\n",
    "print(stayed1['Author'].apply(lambda x: x[:3] == 'AU=').all())\n",
    "print(kicked1['labels'].apply(lambda x: x == 1).all())\n",
    "print(stayed1['labels'].apply(lambda x: x == 0).all())\n",
    "print(kicked1['Title paper'].apply(lambda x: x[0] == '[' and x[-1] == ']').all())\n",
    "print(stayed1['Title paper'].apply(lambda x: x[0] == '[' and x[-1] == ']').all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.concat([kicked1, stayed1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title paper</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AU=AYDIN AYDIN</td>\n",
       "      <td>[Widespread involvement of hepatic, renal and ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AU=AYDIN AYDIN</td>\n",
       "      <td>[Temporal Analysis of Finger-Tapping Test in I...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AU=AYDIN AYDIN</td>\n",
       "      <td>[UNDIFFERENTIATED CARCINOMA OF THE EPIDIDYMIS]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AU=AYDIN AYDIN</td>\n",
       "      <td>[Undifferentiated Primary Carcinoma of the Epi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AU=GİLGİL ERDAL</td>\n",
       "      <td>[Internal iliac artery pseudoaneurysm - An unu...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Author                                        Title paper  labels\n",
       "0   AU=AYDIN AYDIN  [Widespread involvement of hepatic, renal and ...     1.0\n",
       "1   AU=AYDIN AYDIN  [Temporal Analysis of Finger-Tapping Test in I...     1.0\n",
       "2   AU=AYDIN AYDIN     [UNDIFFERENTIATED CARCINOMA OF THE EPIDIDYMIS]     1.0\n",
       "3   AU=AYDIN AYDIN  [Undifferentiated Primary Carcinoma of the Epi...     1.0\n",
       "4  AU=GİLGİL ERDAL  [Internal iliac artery pseudoaneurysm - An unu...     1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title paper</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AYDIN AYDIN</td>\n",
       "      <td>Widespread involvement of hepatic, renal and m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AYDIN AYDIN</td>\n",
       "      <td>Temporal Analysis of Finger-Tapping Test in In...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AYDIN AYDIN</td>\n",
       "      <td>UNDIFFERENTIATED CARCINOMA OF THE EPIDIDYMIS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AYDIN AYDIN</td>\n",
       "      <td>Undifferentiated Primary Carcinoma of the Epid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GİLGİL ERDAL</td>\n",
       "      <td>Internal iliac artery pseudoaneurysm - An unus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Author                                        Title paper  Label\n",
       "0   AYDIN AYDIN  Widespread involvement of hepatic, renal and m...      1\n",
       "1   AYDIN AYDIN  Temporal Analysis of Finger-Tapping Test in In...      1\n",
       "2   AYDIN AYDIN       UNDIFFERENTIATED CARCINOMA OF THE EPIDIDYMIS      1\n",
       "3   AYDIN AYDIN  Undifferentiated Primary Carcinoma of the Epid...      1\n",
       "4  GİLGİL ERDAL  Internal iliac artery pseudoaneurysm - An unus...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df0.copy()\n",
    "df1['Author'] = df0['Author'].apply(lambda x: x[3:])\n",
    "df1['Label'] = df0['labels'].apply(lambda x: int(x))\n",
    "df1['Title paper'] = df0['Title paper'].apply(lambda s: s[1:][:-1])\n",
    "df1 = df1.drop(columns=['labels'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows to remove:\n",
      "7764      ACM\n",
      "22552    SGEM\n",
      "7766      ACM\n",
      "22548    SGEM\n",
      "10549    ASEE\n",
      "4          AU\n",
      "22550    SGEM\n",
      "10548    ASEE\n",
      "3          AU\n",
      "7767      ACM\n",
      "Name: Author, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove trash author names (whose length < 6)\n",
    "print('Rows to remove:')\n",
    "print(df1[df1['Author'].apply(lambda s : len(s) < 6)]['Author'])\n",
    "df2 = df1[df1['Author'].apply(lambda s : len(s) >= 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of not English words:  (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# In fact we will do classification on \"Title paper\", so further working on it\n",
    "print('Shape of not English words: ', df2[df2['Title paper'].apply(lambda s : not isEnglish(s))].shape)\n",
    "df3 = df2.copy()\n",
    "df3['Title paper'] = df2['Title paper'].apply(\n",
    "    lambda s : s.lower()\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"[.,/()?:'%\\\";\\[\\]!\\{\\}><]\", \"\", s) # delete all not-letters\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"[- + = @ & * # |]\", \" \", s) # substitute defis with spaces\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"\\d\", \" \", s) # substitute numbers with spaces\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"\\W\\w{1,2}\\W\", \" \", s) # naive removal of super-short words\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"\\s+\", \" \", s) # substitute multiple spaces with one\n",
    ")\n",
    "df3 = df3[df3['Title paper'].apply(\n",
    "    lambda s: s != 'untitled' and s != 'editorial' # drop some common but not-interesting names\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: Title paper, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "# try to find strange symbols in \"Title paper\" and print them \n",
    "symbols = df3['Title paper'].apply(\n",
    "    lambda s: ''.join(c for c in s if not c.isalpha() and c != ' ')\n",
    ")\n",
    "print(symbols[symbols.apply(lambda s: s != '')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title paper</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>widespread involvement hepatic renal and mesen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>temporal analysis finger tapping test individu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>undifferentiated carcinoma the epididymis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>undifferentiated primary carcinoma the epididy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>internal iliac artery pseudoaneurysm unusual c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Title paper  Label\n",
       "0  widespread involvement hepatic renal and mesen...      1\n",
       "1  temporal analysis finger tapping test individu...      1\n",
       "2          undifferentiated carcinoma the epididymis      1\n",
       "3  undifferentiated primary carcinoma the epididy...      1\n",
       "4  internal iliac artery pseudoaneurysm unusual c...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay, now in df3 in \"Title paper\" we have clean sentences, great, analysis should work\n",
    "\n",
    "df4 = df3.drop(columns=['Author'])\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the interesting observation is that now we have a dataset of (title, label), where label is\n",
    "# if the author of the article was fired or not. Such dataset may be biased, because for one author\n",
    "# there can be a lot of different articles. Thus many points are produced with single observation\n",
    "\n",
    "# Let's try also with another dataset, where we will also have (conc_title, label), where conc_title\n",
    "# will stay for all the titles of one author, being concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Title paper</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AYDIN AYDIN</td>\n",
       "      <td>widespread involvement hepatic renal and mesen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AYDIN AYDIN</td>\n",
       "      <td>temporal analysis finger tapping test individu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AYDIN AYDIN</td>\n",
       "      <td>undifferentiated carcinoma the epididymis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AYDIN AYDIN</td>\n",
       "      <td>undifferentiated primary carcinoma the epididy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GİLGİL ERDAL</td>\n",
       "      <td>internal iliac artery pseudoaneurysm unusual c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Author                                        Title paper  Label\n",
       "0   AYDIN AYDIN  widespread involvement hepatic renal and mesen...      1\n",
       "1   AYDIN AYDIN  temporal analysis finger tapping test individu...      1\n",
       "2   AYDIN AYDIN          undifferentiated carcinoma the epididymis      1\n",
       "3   AYDIN AYDIN  undifferentiated primary carcinoma the epididy...      1\n",
       "4  GİLGİL ERDAL  internal iliac artery pseudoaneurysm unusual c...      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For first dataset we have 24590 titles, from them 12301 kicked and 12289 stayed\n"
     ]
    }
   ],
   "source": [
    "titles_num = df4.shape[0]\n",
    "kicked_titles_num = df4[df4['Label'] == 1].shape[0]\n",
    "stayed_titles_num = df4[df4['Label'] == 0].shape[0]\n",
    "print('For first dataset we have ' + str(titles_num) + ' titles, from them ' + str(kicked_titles_num) + ' kicked and ' + str(stayed_titles_num) + ' stayed')\n",
    "\n",
    "# authors_num = len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After aggregation we got 8094 authors, from which 2757 were kicked and 5337 not\n",
      "Taking prefix of required size for not-kicked\n",
      "Got aggregated dataset of size 5514\n"
     ]
    }
   ],
   "source": [
    "titles_per_author = {} # author -> article\n",
    "labels_per_author = {} # author -> label\n",
    "\n",
    "for i, r in df3.iterrows():\n",
    "    author = r['Author']\n",
    "    title = r['Title paper']\n",
    "    label = int(r['Label'])\n",
    "    \n",
    "    titles_per_author[author] = titles_per_author.get(author, '') + ' ' + title # do concatenation\n",
    "    labels_per_author[author] = label\n",
    "\n",
    "kicked_cnt = 0\n",
    "for k, v in labels_per_author.items():\n",
    "    if v == 1: kicked_cnt+= 1\n",
    "        \n",
    "print('After aggregation we got ' + str(len(titles_per_author)) + ' authors, from which ' + str(kicked_cnt) + ' were kicked and ' + str(len(titles_per_author) - kicked_cnt) + ' not')\n",
    "print('Taking prefix of required size for not-kicked')\n",
    "\n",
    "authors = []\n",
    "titles = []\n",
    "labels = []\n",
    "stayed_limit = kicked_cnt\n",
    "\n",
    "for k, v in titles_per_author.items():\n",
    "    if labels_per_author[k] == 0:\n",
    "        if stayed_limit > 0: stayed_limit -= 1\n",
    "        else: continue\n",
    "    \n",
    "    authors.append(k)\n",
    "    titles.append(re.sub(r\"\\s+\", \" \", v))    \n",
    "    labels.append(labels_per_author[k])\n",
    "    \n",
    "# aggregated DataFrame\n",
    "adf4 = pd.DataFrame(data={'Title paper' : titles, 'Label' : labels}) # columns names are ugly, but for backwards-compatibility\n",
    "print('Got aggregated dataset of size ' + str(len(authors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5514 unique titles in dataset: 5479\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size: ' + str(adf4.shape[0]) + ' unique titles in dataset: ' + str(len(set(adf4['Title paper']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x[1] == 2, Counter(adf4['Title paper']).items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x[1] > 2, Counter(adf4['Title paper']).items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_per_titles = {}\n",
    "same_duplicate = 0\n",
    "diff_duplicate = 0\n",
    "\n",
    "# we assume, that noone meets 3 times, which +- correct\n",
    "for i, r in adf4.iterrows():\n",
    "    title = r['Title paper']\n",
    "    label = int(r['Label'])\n",
    "    if title in labels_per_titles:\n",
    "        if labels_per_titles[title] == label:\n",
    "            same_duplicate += 1\n",
    "        else:\n",
    "            diff_duplicate += 1\n",
    "    else:\n",
    "        labels_per_titles[title] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 8)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_duplicate, diff_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset size after duplicates removal is 5471\n"
     ]
    }
   ],
   "source": [
    "# to clean the data, let's throw away all the duplicates at all, both same and diff, everyone, who meets > 1 times\n",
    "counter = Counter(adf4['Title paper'])\n",
    "\n",
    "adf5 = adf4[\n",
    "    adf4['Title paper'].apply(\n",
    "        lambda title: counter[title] == 1\n",
    "    )\n",
    "]\n",
    "\n",
    "print('New dataset size after duplicates removal is ' + str(adf5.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3812,), (1634,))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df4\n",
    "df = adf5\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=0.3) # random_state = 0\n",
    "\n",
    "X_train = data_train['Title paper']\n",
    "y_train = data_train['Label']\n",
    "\n",
    "X_test = data_test['Title paper']\n",
    "y_test = data_test['Label']\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the following model\n",
    "# Features are unique words\n",
    "# Samples are titles\n",
    "\n",
    "# 1) Naive : for every sample we have binary value for every word (present / absent)\n",
    "# 2) sklearn.CountVectorizer : counting\n",
    "# 3) sklearn.TfidfVectorizer : with usual counting more weight is given to longer sentences, that's not really\n",
    "#                               fair, TF-IDF (term frequency _times_ inverse document frequency) also gives\n",
    "#                               every sample a weight for each present word, but in more sophisticated way\n",
    "\n",
    "# We are doing (3) classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (3812, 18801)\n",
      "Test  shape:  (1634, 18801)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "print('Train shape: ', X_train_tfidf.shape)\n",
    "\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "print('Test  shape: ', X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.572827417380661"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf.predict(X_test_tfidf) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec experiments + catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, we had a look at NB combined with TF-IDF\n",
    "# Let's now work with word2vec. We need to handle sentences, thus we have 2 options:\n",
    "# 1) Do simple averaging of all the words in sentence\n",
    "# 2) Do TF-IDF weighting of every word in a sentence and then addition\n",
    "# Then for classification we use catboost (ie gradient boosting ie combination of decision trees)\n",
    "\n",
    "# We will try both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pretrained model\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2735 vs test size: 2736\n"
     ]
    }
   ],
   "source": [
    "# df = df4\n",
    "df = adf5\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=0.5) # random_state = 0\n",
    "\n",
    "X_train = data_train['Title paper']\n",
    "y_train = data_train['Label']\n",
    "\n",
    "X_test = data_test['Title paper']\n",
    "y_test = data_test['Label']\n",
    "\n",
    "print('Train size: ' + str(X_train.shape[0]) + ' vs test size: ' + str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Title paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the relation general anxiety levels anxiety w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>comparison sugammadex and neostigmine atropin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>protective effect curcumin carbapenem resista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>paratesticular tumors and clinicopathologic a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>re based determination ureteral stone volume ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>comparison and validation scoring systems a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>acute myocardial infarction improving ventric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>continuous passive motion adhesive capsulitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>commercial and ethnic uses satureja sivri kek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>evaluation the bond strength different adhesi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>prevalence epilepsy bursa city center urban a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>evaluation some berry quality characteristics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>late side effects high dose steroid therapy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>effects contemporary orthodontic composites t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>war and humour according the theory superiori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>acute myocardial infarction improving ventric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>structural complexity analysis cerebellum chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>the use social media and popular culture adva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>histological response injected dextranomer ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>geotechnical problems experienced during the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>drainage and mechanical behavior highway base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>examining turkish social studies teachers bel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>is there relationship between immune mediated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>association nfkb and micrornas variations and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>the most frequent writing errors students lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>characterization a novel zebrafish danio reri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>mathematical modelling a potential tsunami as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>early changes atrial conduction times hyperte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>survey wild food plants for human consumption...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>general characteristics villages living rural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>achieving continuous performance a dual proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>complementary and alternative treatment metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>is the gok atlas sufficiently reliable for fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>effect learning stysles physiology education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>analysis outcomes based left ventricular ejec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>possible association between hormonal status ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>an integrated fuzzy ahp and topsis technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>from endometrial hyperplasia endometrial canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>salicylate intoxication masquerading diabetic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>relationship between arterial stiffness and m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>profuse erythema multiforme induced chlorambu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>development a questionnaire assess inter epis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>progesterone and preterm birth reply an advan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>integral analysis boundary layer flows with p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>special proper pointwise slant surfaces a loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>object recognition with generic self organizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>the effect transforming growth factor beta tg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>chromatographic separation and analytic proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>fractional integral associated with schroding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>conjunctival myxoma clinicopathologic report ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Label                                        Title paper\n",
       "0       1   the relation general anxiety levels anxiety w...\n",
       "1       1   comparison sugammadex and neostigmine atropin...\n",
       "2       1   protective effect curcumin carbapenem resista...\n",
       "3       0   paratesticular tumors and clinicopathologic a...\n",
       "4       0   re based determination ureteral stone volume ...\n",
       "5       0   comparison and validation scoring systems a c...\n",
       "6       0   acute myocardial infarction improving ventric...\n",
       "7       0   continuous passive motion adhesive capsulitis...\n",
       "8       0   commercial and ethnic uses satureja sivri kek...\n",
       "9       0   evaluation the bond strength different adhesi...\n",
       "10      0   prevalence epilepsy bursa city center urban a...\n",
       "11      0   evaluation some berry quality characteristics...\n",
       "12      0   late side effects high dose steroid therapy s...\n",
       "13      1   effects contemporary orthodontic composites t...\n",
       "14      1   war and humour according the theory superiori...\n",
       "15      0   acute myocardial infarction improving ventric...\n",
       "16      0   structural complexity analysis cerebellum chi...\n",
       "17      1   the use social media and popular culture adva...\n",
       "18      0   histological response injected dextranomer ba...\n",
       "19      0   geotechnical problems experienced during the ...\n",
       "20      0   drainage and mechanical behavior highway base...\n",
       "21      1   examining turkish social studies teachers bel...\n",
       "22      1   is there relationship between immune mediated...\n",
       "23      0   association nfkb and micrornas variations and...\n",
       "24      1   the most frequent writing errors students lea...\n",
       "25      0   characterization a novel zebrafish danio reri...\n",
       "26      0   mathematical modelling a potential tsunami as...\n",
       "27      0   early changes atrial conduction times hyperte...\n",
       "28      0   survey wild food plants for human consumption...\n",
       "29      1   general characteristics villages living rural...\n",
       "30      0   achieving continuous performance a dual proce...\n",
       "31      0   complementary and alternative treatment metho...\n",
       "32      1   is the gok atlas sufficiently reliable for fo...\n",
       "33      0       effect learning stysles physiology education\n",
       "34      0   analysis outcomes based left ventricular ejec...\n",
       "35      0   possible association between hormonal status ...\n",
       "36      1   an integrated fuzzy ahp and topsis technique ...\n",
       "37      0   from endometrial hyperplasia endometrial canc...\n",
       "38      0   salicylate intoxication masquerading diabetic...\n",
       "39      1   relationship between arterial stiffness and m...\n",
       "40      0   profuse erythema multiforme induced chlorambu...\n",
       "41      0   development a questionnaire assess inter epis...\n",
       "42      0   progesterone and preterm birth reply an advan...\n",
       "43      0   integral analysis boundary layer flows with p...\n",
       "44      1   special proper pointwise slant surfaces a loc...\n",
       "45      1   object recognition with generic self organizi...\n",
       "46      1   the effect transforming growth factor beta tg...\n",
       "47      1   chromatographic separation and analytic proce...\n",
       "48      0   fractional integral associated with schroding...\n",
       "49      1   conjunctival myxoma clinicopathologic report ..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_w2v_embeddings(titles):\n",
    "    embs = []\n",
    "    for title in titles:\n",
    "        title_emb = np.zeros(300)\n",
    "        words = title.split(' ')\n",
    "        for w in words:\n",
    "            if w in word2vec_model:\n",
    "                scalar = 1.\n",
    "#                 scalar = 1. / len(words)\n",
    "                \n",
    "                vector = word2vec_model[w]\n",
    "                \n",
    "                title_emb += scalar * vector\n",
    "        embs.append(title_emb)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Series is provided, indexation is made with iloc\n",
    "def get_tfidf_w2v_embeddings(titles):\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    \n",
    "    titles_tfidf_matrix = tfidf_vect.fit_transform(titles)\n",
    "    # have matrix, where rows are titles and cols are words from vocabulary\n",
    "    tfidf_words_indices = {word : index for (word, index) in tfidf_vect.vocabulary_.items()}\n",
    "    \n",
    "    embs = []\n",
    "    for i in range(len(titles)):\n",
    "        title = titles.iloc[i]\n",
    "        words = title.split(' ')\n",
    "        \n",
    "        # make sparse matrix row a dict:\n",
    "        matrix_row = titles_tfidf_matrix[i]\n",
    "        matrix_row_dict = {}\n",
    "        indices = matrix_row.indices\n",
    "        data    = matrix_row.data\n",
    "        for i in range(len(data)):\n",
    "            matrix_row_dict[indices[i]] = data[i]\n",
    "        \n",
    "        title_emb = np.zeros(300)\n",
    "        for w in words:\n",
    "            if w in word2vec_model:\n",
    "                vector = word2vec_model[w]\n",
    "                \n",
    "                if w in tfidf_words_indices:\n",
    "                    word_index = tfidf_words_indices[w]\n",
    "                    scalar = matrix_row_dict.get(word_index, 0)\n",
    "                else:\n",
    "                    scalar = 1. / len(words) # take scalar as in mean\n",
    "#                     scalar = 1.\n",
    "                \n",
    "                title_emb += scalar * vector\n",
    "                \n",
    "        embs.append(title_emb)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    X_train_embs = get_mean_w2v_embeddings(X_train)\n",
    "    X_test_embs  = get_mean_w2v_embeddings(X_test)\n",
    "else:\n",
    "    X_train_embs = get_tfidf_w2v_embeddings(X_train)\n",
    "    X_test_embs  = get_tfidf_w2v_embeddings(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6895761\ttotal: 166ms\tremaining: 3.16s\n",
      "1:\tlearn: 0.6863437\ttotal: 260ms\tremaining: 2.34s\n",
      "2:\tlearn: 0.6828959\ttotal: 354ms\tremaining: 2.01s\n",
      "3:\tlearn: 0.6792450\ttotal: 450ms\tremaining: 1.8s\n",
      "4:\tlearn: 0.6757360\ttotal: 541ms\tremaining: 1.62s\n",
      "5:\tlearn: 0.6726710\ttotal: 631ms\tremaining: 1.47s\n",
      "6:\tlearn: 0.6691413\ttotal: 728ms\tremaining: 1.35s\n",
      "7:\tlearn: 0.6659759\ttotal: 824ms\tremaining: 1.24s\n",
      "8:\tlearn: 0.6629188\ttotal: 920ms\tremaining: 1.12s\n",
      "9:\tlearn: 0.6600135\ttotal: 1.02s\tremaining: 1.02s\n",
      "10:\tlearn: 0.6569584\ttotal: 1.11s\tremaining: 913ms\n",
      "11:\tlearn: 0.6539126\ttotal: 1.23s\tremaining: 818ms\n",
      "12:\tlearn: 0.6510188\ttotal: 1.33s\tremaining: 717ms\n",
      "13:\tlearn: 0.6481151\ttotal: 1.42s\tremaining: 609ms\n",
      "14:\tlearn: 0.6453650\ttotal: 1.52s\tremaining: 506ms\n",
      "15:\tlearn: 0.6428302\ttotal: 1.61s\tremaining: 403ms\n",
      "16:\tlearn: 0.6399679\ttotal: 1.71s\tremaining: 301ms\n",
      "17:\tlearn: 0.6374751\ttotal: 1.8s\tremaining: 200ms\n",
      "18:\tlearn: 0.6346658\ttotal: 1.91s\tremaining: 101ms\n",
      "19:\tlearn: 0.6316207\ttotal: 2.01s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7368421052631579"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cbc_model = CatBoostClassifier(iterations=20, learning_rate=0.01, depth=6, loss_function='Logloss')\n",
    "cbc_model.fit(X_train_embs, y_train)\n",
    "preds_class = cbc_model.predict(X_test_embs)\n",
    "\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6488069\ttotal: 186ms\tremaining: 3.54s\n",
      "1:\tlearn: 0.6178566\ttotal: 362ms\tremaining: 3.25s\n",
      "2:\tlearn: 0.5906073\ttotal: 551ms\tremaining: 3.12s\n",
      "3:\tlearn: 0.5659806\ttotal: 768ms\tremaining: 3.07s\n",
      "4:\tlearn: 0.5457881\ttotal: 1.02s\tremaining: 3.07s\n",
      "5:\tlearn: 0.5305273\ttotal: 1.25s\tremaining: 2.91s\n",
      "6:\tlearn: 0.5183062\ttotal: 1.46s\tremaining: 2.72s\n",
      "7:\tlearn: 0.5052012\ttotal: 1.66s\tremaining: 2.48s\n",
      "8:\tlearn: 0.4938492\ttotal: 1.85s\tremaining: 2.26s\n",
      "9:\tlearn: 0.4824432\ttotal: 2.03s\tremaining: 2.03s\n",
      "10:\tlearn: 0.4730168\ttotal: 2.25s\tremaining: 1.84s\n",
      "11:\tlearn: 0.4639888\ttotal: 2.43s\tremaining: 1.62s\n",
      "12:\tlearn: 0.4575706\ttotal: 2.63s\tremaining: 1.41s\n",
      "13:\tlearn: 0.4524765\ttotal: 2.8s\tremaining: 1.2s\n",
      "14:\tlearn: 0.4479556\ttotal: 3s\tremaining: 999ms\n",
      "15:\tlearn: 0.4416208\ttotal: 3.19s\tremaining: 798ms\n",
      "16:\tlearn: 0.4361867\ttotal: 3.41s\tremaining: 601ms\n",
      "17:\tlearn: 0.4308438\ttotal: 3.6s\tremaining: 399ms\n",
      "18:\tlearn: 0.4252953\ttotal: 3.78s\tremaining: 199ms\n",
      "19:\tlearn: 0.4210432\ttotal: 3.96s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8047735618115055"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbc_model = CatBoostClassifier(iterations=20, learning_rate=0.1, depth=6, loss_function='Logloss')\n",
    "cbc_model.fit(X_train_embs, y_train)\n",
    "preds_class = cbc_model.predict(X_test_embs)\n",
    "\n",
    "np.mean(preds_class == y_test)model = CatBoostClassifier(iterations=20, learning_rate=0.1, depth=6, loss_function='Logloss')\n",
    "cbc_\n",
    "\n",
    "#Fast text classifier \n",
    "\n",
    "#Which words (key words) --> interpretation  \n",
    "\n",
    "#Cluster vextors -> wich words represent well each cathegory? (sum up all words in one cathegory, \n",
    "·# which words are close to the resutl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4879080\ttotal: 189ms\tremaining: 3.59s\n",
      "1:\tlearn: 0.4557720\ttotal: 367ms\tremaining: 3.3s\n",
      "2:\tlearn: 0.4416080\ttotal: 553ms\tremaining: 3.13s\n",
      "3:\tlearn: 0.4268077\ttotal: 774ms\tremaining: 3.1s\n",
      "4:\tlearn: 0.4209252\ttotal: 956ms\tremaining: 2.87s\n",
      "5:\tlearn: 0.4108938\ttotal: 1.14s\tremaining: 2.66s\n",
      "6:\tlearn: 0.4003444\ttotal: 1.35s\tremaining: 2.51s\n",
      "7:\tlearn: 0.3917893\ttotal: 1.58s\tremaining: 2.37s\n",
      "8:\tlearn: 0.3860356\ttotal: 1.79s\tremaining: 2.19s\n",
      "9:\tlearn: 0.3837082\ttotal: 1.98s\tremaining: 1.98s\n",
      "10:\tlearn: 0.3680311\ttotal: 2.16s\tremaining: 1.77s\n",
      "11:\tlearn: 0.3602340\ttotal: 2.33s\tremaining: 1.55s\n",
      "12:\tlearn: 0.3500398\ttotal: 2.54s\tremaining: 1.37s\n",
      "13:\tlearn: 0.3467383\ttotal: 2.72s\tremaining: 1.17s\n",
      "14:\tlearn: 0.3326018\ttotal: 2.9s\tremaining: 967ms\n",
      "15:\tlearn: 0.3282389\ttotal: 3.08s\tremaining: 770ms\n",
      "16:\tlearn: 0.3276505\ttotal: 3.28s\tremaining: 580ms\n",
      "17:\tlearn: 0.3215752\ttotal: 3.46s\tremaining: 384ms\n",
      "18:\tlearn: 0.3166910\ttotal: 3.64s\tremaining: 192ms\n",
      "19:\tlearn: 0.3103794\ttotal: 3.8s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7772337821297429"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbc_model = CatBoostClassifier(iterations=20, learning_rate=1, depth=6, loss_function='Logloss')\n",
    "cbc_model.fit(X_train_embs, y_train)\n",
    "preds_class = cbc_model.predict(X_test_embs)\n",
    "\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sent2vec + catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec # epfl-made \n",
    "sent2vec_model = sent2vec.Sent2vecModel()\n",
    "sent2vec_model.load_model('./data/pretrained_models/wiki_unigrams.bin') # 600 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3812,), (1634,))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df4\n",
    "df = adf5\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=0.3) # random_state = 0\n",
    "\n",
    "X_train = data_train['Title paper']\n",
    "y_train = data_train['Label']\n",
    "\n",
    "X_test = data_test['Title paper']\n",
    "y_test = data_test['Label']\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = model.embed_sentence(X_train.values[0])\n",
    "X_train_embs = sent2vec_model.embed_sentences(X_train.values)\n",
    "X_test_embs  = sent2vec_model.embed_sentences(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3812, numpy.ndarray, 600, numpy.float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_embs), type(X_train_embs[0]), len(X_train_embs[0]), type(X_train_embs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, now we have 600-dim vectors for every sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6921994\ttotal: 353ms\tremaining: 6.7s\n",
      "1:\tlearn: 0.6913647\ttotal: 713ms\tremaining: 6.41s\n",
      "2:\tlearn: 0.6905139\ttotal: 1.09s\tremaining: 6.2s\n",
      "3:\tlearn: 0.6895561\ttotal: 1.43s\tremaining: 5.73s\n",
      "4:\tlearn: 0.6886990\ttotal: 1.81s\tremaining: 5.44s\n",
      "5:\tlearn: 0.6876661\ttotal: 2.17s\tremaining: 5.07s\n",
      "6:\tlearn: 0.6869585\ttotal: 2.52s\tremaining: 4.68s\n",
      "7:\tlearn: 0.6860869\ttotal: 2.87s\tremaining: 4.31s\n",
      "8:\tlearn: 0.6853812\ttotal: 3.22s\tremaining: 3.94s\n",
      "9:\tlearn: 0.6844824\ttotal: 3.58s\tremaining: 3.58s\n",
      "10:\tlearn: 0.6837446\ttotal: 3.92s\tremaining: 3.21s\n",
      "11:\tlearn: 0.6828041\ttotal: 4.28s\tremaining: 2.86s\n",
      "12:\tlearn: 0.6819210\ttotal: 4.63s\tremaining: 2.49s\n",
      "13:\tlearn: 0.6811049\ttotal: 4.99s\tremaining: 2.14s\n",
      "14:\tlearn: 0.6801637\ttotal: 5.32s\tremaining: 1.77s\n",
      "15:\tlearn: 0.6793613\ttotal: 5.67s\tremaining: 1.42s\n",
      "16:\tlearn: 0.6784833\ttotal: 6.02s\tremaining: 1.06s\n",
      "17:\tlearn: 0.6776294\ttotal: 6.38s\tremaining: 708ms\n",
      "18:\tlearn: 0.6768629\ttotal: 6.73s\tremaining: 354ms\n",
      "19:\tlearn: 0.6761846\ttotal: 7.08s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6976744186046512"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=20, learning_rate=1e-2, depth=6, loss_function='Logloss')\n",
    "model.fit(X_train_embs, y_train)\n",
    "preds_class = model.predict(X_test_embs)\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6841627\ttotal: 327ms\tremaining: 6.21s\n",
      "1:\tlearn: 0.6771905\ttotal: 697ms\tremaining: 6.28s\n",
      "2:\tlearn: 0.6681140\ttotal: 1.05s\tremaining: 5.97s\n",
      "3:\tlearn: 0.6613314\ttotal: 1.4s\tremaining: 5.59s\n",
      "4:\tlearn: 0.6548034\ttotal: 1.74s\tremaining: 5.23s\n",
      "5:\tlearn: 0.6469348\ttotal: 2.09s\tremaining: 4.88s\n",
      "6:\tlearn: 0.6404247\ttotal: 2.44s\tremaining: 4.53s\n",
      "7:\tlearn: 0.6344413\ttotal: 2.79s\tremaining: 4.18s\n",
      "8:\tlearn: 0.6296052\ttotal: 3.15s\tremaining: 3.84s\n",
      "9:\tlearn: 0.6247514\ttotal: 3.51s\tremaining: 3.51s\n",
      "10:\tlearn: 0.6205616\ttotal: 3.86s\tremaining: 3.16s\n",
      "11:\tlearn: 0.6145091\ttotal: 4.22s\tremaining: 2.81s\n",
      "12:\tlearn: 0.6105035\ttotal: 4.57s\tremaining: 2.46s\n",
      "13:\tlearn: 0.6061513\ttotal: 4.93s\tremaining: 2.11s\n",
      "14:\tlearn: 0.6006972\ttotal: 5.27s\tremaining: 1.75s\n",
      "15:\tlearn: 0.5974907\ttotal: 5.61s\tremaining: 1.4s\n",
      "16:\tlearn: 0.5939123\ttotal: 5.95s\tremaining: 1.05s\n",
      "17:\tlearn: 0.5892206\ttotal: 6.31s\tremaining: 701ms\n",
      "18:\tlearn: 0.5849957\ttotal: 6.66s\tremaining: 351ms\n",
      "19:\tlearn: 0.5812133\ttotal: 7.01s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7074663402692778"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=20, learning_rate=1e-1, depth=6, loss_function='Logloss')\n",
    "model.fit(X_train_embs, y_train)\n",
    "preds_class = model.predict(X_test_embs)\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6477382\ttotal: 345ms\tremaining: 6.55s\n",
      "1:\tlearn: 0.6258880\ttotal: 772ms\tremaining: 6.95s\n",
      "2:\tlearn: 0.6064956\ttotal: 1.16s\tremaining: 6.59s\n",
      "3:\tlearn: 0.5822261\ttotal: 1.52s\tremaining: 6.07s\n",
      "4:\tlearn: 0.5660781\ttotal: 1.86s\tremaining: 5.58s\n",
      "5:\tlearn: 0.5494321\ttotal: 2.2s\tremaining: 5.13s\n",
      "6:\tlearn: 0.5370378\ttotal: 2.54s\tremaining: 4.71s\n",
      "7:\tlearn: 0.5209341\ttotal: 2.9s\tremaining: 4.34s\n",
      "8:\tlearn: 0.5080571\ttotal: 3.25s\tremaining: 3.97s\n",
      "9:\tlearn: 0.4923026\ttotal: 3.62s\tremaining: 3.62s\n",
      "10:\tlearn: 0.4865990\ttotal: 3.93s\tremaining: 3.22s\n",
      "11:\tlearn: 0.4693939\ttotal: 4.29s\tremaining: 2.86s\n",
      "12:\tlearn: 0.4669019\ttotal: 4.59s\tremaining: 2.47s\n",
      "13:\tlearn: 0.4524012\ttotal: 4.94s\tremaining: 2.12s\n",
      "14:\tlearn: 0.4453808\ttotal: 5.28s\tremaining: 1.76s\n",
      "15:\tlearn: 0.4347320\ttotal: 5.65s\tremaining: 1.41s\n",
      "16:\tlearn: 0.4196183\ttotal: 6s\tremaining: 1.06s\n",
      "17:\tlearn: 0.4137137\ttotal: 6.35s\tremaining: 706ms\n",
      "18:\tlearn: 0.4097078\ttotal: 6.68s\tremaining: 352ms\n",
      "19:\tlearn: 0.3878707\ttotal: 7.05s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6719706242350061"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=20, learning_rate=1, depth=6, loss_function='Logloss')\n",
    "model.fit(X_train_embs, y_train)\n",
    "preds_class = model.predict(X_test_embs)\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.5727963\ttotal: 328ms\tremaining: 6.22s\n",
      "1:\tlearn: 536.3227302\ttotal: 676ms\tremaining: 6.09s\n",
      "2:\tlearn: nan\ttotal: 699ms\tremaining: 3.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training has stopped (degenerate solution on iteration 2, probably too small l2-regularization, try to increase it)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5416156670746634"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=20, learning_rate=10, depth=6, loss_function='Logloss')\n",
    "model.fit(X_train_embs, y_train)\n",
    "preds_class = model.predict(X_test_embs)\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
