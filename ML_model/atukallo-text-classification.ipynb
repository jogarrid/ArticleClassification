{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import mpl_toolkits.mplot3d as mplt3d\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# shouldn't be enabled when using interactive 3D plots\n",
    "# %pylab inline\n",
    "# pylab.rcParams['figure.figsize'] = (10, 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning & preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming chosen for same-length, to look pretty\n",
    "kicked = pd.read_csv('../data/DISMISSED_final.csv')\n",
    "stayed = pd.read_csv('../data/UNDISMISSED_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kicked.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stayed.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kicked.shape, stayed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kicked1 = kicked[['author','title', 'labels']]\n",
    "stayed1 = stayed[['author','title', 'labels']]\n",
    "\n",
    "stayed1 = stayed1.sample(frac=(1.0 * kicked.shape[0])/stayed.shape[0]) # random_state = 0\n",
    "\n",
    "# made arrays equal in size\n",
    "kicked1.shape, stayed1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some basic invariants on the input data, all should return True\n",
    "print(kicked1['labels'].apply(lambda x: x == 1).all())\n",
    "print(stayed1['labels'].apply(lambda x: x == 0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.concat([kicked1, stayed1])\n",
    "df1 = df0.copy()\n",
    "df1['labels'] = df0['labels'].apply(lambda x: int(x))\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In fact we will do classification on \"Title paper\", so further working on it\n",
    "print('Shape of not English words: ', df2[df2['title'].apply(lambda s : not isEnglish(s))].shape)\n",
    "df3 = df2.copy()\n",
    "df3['title'] = df2['title'].apply(\n",
    "    lambda s : s.lower()\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"[.,/()?:'%\\\";\\[\\]!\\{\\}><\\\\_]\", \"\", s) # delete all not-letters\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"[- + = @ & * # |]\", \" \", s) # substitute defis with spaces\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"\\d\", \" \", s) # substitute numbers with spaces\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"\\W\\w{1,2}\\W\", \" \", s) # naive removal of super-short words\n",
    ").apply(\n",
    "    lambda s : re.sub(r\"\\s+\", \" \", s) # substitute multiple spaces with one\n",
    ")\n",
    "df3 = df3[df3['title'].apply(\n",
    "    lambda s: s != 'untitled' and s != 'editorial' # drop some common but not-interesting names\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find strange symbols in \"Title paper\" and print them \n",
    "symbols = df3['title'].apply(\n",
    "    lambda s: ''.join(c for c in s if not c.isalpha() and c != ' ')\n",
    ")\n",
    "print(symbols[symbols.apply(lambda s: s != '')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get (title, label) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, now in df3 in \"Title paper\" we have clean sentences, great, analysis should work\n",
    "df4 = df3.drop(columns=['author'])\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get (concatenated-titles-per-author, label) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the interesting observation is that now we have a dataset of (title, label), where label is\n",
    "# if the author of the article was fired or not. Such dataset may be biased, because for one author\n",
    "# there can be a lot of different articles. Thus many points are produced with single observation\n",
    "\n",
    "# Let's try also with another dataset, where we will also have (conc_title, label), where conc_title\n",
    "# will stay for all the titles of one author, being concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_num = df4.shape[0]\n",
    "kicked_titles_num = df4[df4['labels'] == 1].shape[0]\n",
    "stayed_titles_num = df4[df4['labels'] == 0].shape[0]\n",
    "print('For first dataset we have ' + str(titles_num) + ' titles, from them ' + str(kicked_titles_num) + ' kicked and ' + str(stayed_titles_num) + ' stayed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_per_author = {} # author -> article\n",
    "labels_per_author = {} # author -> label\n",
    "\n",
    "for i, r in df3.iterrows():\n",
    "    author = r['author']\n",
    "    title = r['title']\n",
    "    label = int(r['labels'])\n",
    "    \n",
    "    titles_per_author[author] = titles_per_author.get(author, '') + ' ' + title # do concatenation\n",
    "    labels_per_author[author] = label\n",
    "\n",
    "kicked_cnt = 0\n",
    "for k, v in labels_per_author.items():\n",
    "    if v == 1: kicked_cnt+= 1\n",
    "        \n",
    "print('After aggregation we got ' + str(len(titles_per_author)) + ' authors, from which ' + str(kicked_cnt) + ' were kicked and ' + str(len(titles_per_author) - kicked_cnt) + ' not')\n",
    "print('Taking prefix of required size for not-kicked')\n",
    "\n",
    "authors = []\n",
    "titles = []\n",
    "labels = []\n",
    "stayed_limit = kicked_cnt\n",
    "\n",
    "for k, v in titles_per_author.items():\n",
    "    if labels_per_author[k] == 0:\n",
    "        if stayed_limit > 0: stayed_limit -= 1\n",
    "        else: continue\n",
    "    \n",
    "    authors.append(k)\n",
    "    titles.append(re.sub(r\"\\s+\", \" \", v))    \n",
    "    labels.append(labels_per_author[k])\n",
    "    \n",
    "# aggregated DataFrame\n",
    "adf4 = pd.DataFrame(data={'title' : titles, 'labels' : labels})\n",
    "print('Got aggregated dataset of size ' + str(len(authors)))\n",
    "\n",
    "# to clean the data, let's throw away all the duplicates at all, both same and diff, everyone, who meets > 1 times\n",
    "counter = Counter(adf4['title'])\n",
    "adf5 = adf4[\n",
    "    adf4['title'].apply(\n",
    "        lambda title: counter[title] == 1\n",
    "    )\n",
    "]\n",
    "\n",
    "print('New dataset size after duplicates removal is ' + str(adf5.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df4\n",
    "df = adf5\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=0.3) # random_state = 0\n",
    "\n",
    "X_train = data_train['title']\n",
    "y_train = data_train['labels']\n",
    "\n",
    "X_test = data_test['title']\n",
    "y_test = data_test['labels']\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the following model\n",
    "# Features are unique words\n",
    "# Samples are titles\n",
    "\n",
    "# 1) Naive : for every sample we have binary value for every word (present / absent)\n",
    "# 2) sklearn.CountVectorizer : counting\n",
    "# 3) sklearn.TfidfVectorizer : with usual counting more weight is given to longer sentences, that's not really\n",
    "#                               fair, TF-IDF (term frequency _times_ inverse document frequency) also gives\n",
    "#                               every sample a weight for each present word, but in more sophisticated way\n",
    "\n",
    "# We are doing (3) classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "print('Train shape: ', X_train_tfidf.shape)\n",
    "\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "print('Test  shape: ', X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(clf.predict(X_test_tfidf) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec experiments + catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, we had a look at NB combined with TF-IDF\n",
    "# Let's now work with word2vec. We need to handle sentences, thus we have 2 options:\n",
    "# 1) Do simple averaging of all the words in sentence\n",
    "# 2) Do TF-IDF weighting of every word in a sentence and then addition\n",
    "# Then for classification we use catboost (ie gradient boosting ie combination of decision trees)\n",
    "\n",
    "# We will try both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pretrained model\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../data/pretrained_models/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df4\n",
    "df = adf5\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=0.3) # random_state = 0\n",
    "\n",
    "X_train = data_train['title']\n",
    "y_train = data_train['labels']\n",
    "\n",
    "X_test = data_test['title']\n",
    "y_test = data_test['labels']\n",
    "\n",
    "print('Train size: ' + str(X_train.shape[0]) + ' vs test size: ' + str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_w2v_embeddings(titles):\n",
    "    embs = []\n",
    "    for title in titles:\n",
    "        title_emb = np.zeros(300)\n",
    "        words = title.split(' ')\n",
    "        for w in words:\n",
    "            if w in word2vec_model:\n",
    "                scalar = 1.\n",
    "#                 scalar = 1. / len(words)\n",
    "                \n",
    "                vector = word2vec_model[w]\n",
    "                \n",
    "                title_emb += scalar * vector\n",
    "        embs.append(title_emb)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Series is provided, indexation is made with iloc\n",
    "def get_tfidf_w2v_embeddings(titles):\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    \n",
    "    titles_tfidf_matrix = tfidf_vect.fit_transform(titles)\n",
    "    # have matrix, where rows are titles and cols are words from vocabulary\n",
    "    tfidf_words_indices = {word : index for (word, index) in tfidf_vect.vocabulary_.items()}\n",
    "    \n",
    "    embs = []\n",
    "    for i in range(len(titles)):\n",
    "        title = titles.iloc[i]\n",
    "        words = title.split(' ')\n",
    "        \n",
    "        # make sparse matrix row a dict:\n",
    "        matrix_row = titles_tfidf_matrix[i]\n",
    "        matrix_row_dict = {}\n",
    "        indices = matrix_row.indices\n",
    "        data    = matrix_row.data\n",
    "        for i in range(len(data)):\n",
    "            matrix_row_dict[indices[i]] = data[i]\n",
    "        \n",
    "        title_emb = np.zeros(300)\n",
    "        for w in words:\n",
    "            if w in word2vec_model:\n",
    "                vector = word2vec_model[w]\n",
    "                \n",
    "                if w in tfidf_words_indices:\n",
    "                    word_index = tfidf_words_indices[w]\n",
    "                    scalar = matrix_row_dict.get(word_index, 0)\n",
    "                else:\n",
    "                    scalar = 1. / len(words) # take scalar as in mean\n",
    "#                     scalar = 1.\n",
    "                \n",
    "                title_emb += scalar * vector\n",
    "                \n",
    "        embs.append(title_emb)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    X_train_embs = get_mean_w2v_embeddings(X_train)\n",
    "    X_test_embs  = get_mean_w2v_embeddings(X_test)\n",
    "else:\n",
    "    X_train_embs = get_tfidf_w2v_embeddings(X_train)\n",
    "    X_test_embs  = get_tfidf_w2v_embeddings(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cbc_model = CatBoostClassifier(iterations=20, learning_rate=0.01, depth=6, loss_function='Logloss')\n",
    "cbc_model.fit(X_train_embs, y_train)\n",
    "preds_class = cbc_model.predict(X_test_embs)\n",
    "\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_model = CatBoostClassifier(iterations=20, learning_rate=0.1, depth=6, loss_function='Logloss')\n",
    "cbc_model.fit(X_train_embs, y_train)\n",
    "preds_class = cbc_model.predict(X_test_embs)\n",
    "\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_model = CatBoostClassifier(iterations=20, learning_rate=1, depth=6, loss_function='Logloss')\n",
    "cbc_model.fit(X_train_embs, y_train)\n",
    "preds_class = cbc_model.predict(X_test_embs)\n",
    "\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_model = CatBoostClassifier(iterations=20, learning_rate=10, depth=6, loss_function='Logloss')\n",
    "cbc_model.fit(X_train_embs, y_train)\n",
    "preds_class = cbc_model.predict(X_test_embs)\n",
    "\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sent2vec + catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sent2vec # epfl-made \n",
    "sent2vec_model = sent2vec.Sent2vecModel()\n",
    "sent2vec_model.load_model('../data/pretrained_models/wiki_unigrams.bin') # 600 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df4\n",
    "df = adf5\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=0.3) # random_state = 0\n",
    "\n",
    "X_train = data_train['title']\n",
    "y_train = data_train['labels']\n",
    "\n",
    "X_test = data_test['title']\n",
    "y_test = data_test['labels']\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = model.embed_sentence(X_train.values[0])\n",
    "X_train_embs = sent2vec_model.embed_sentences(X_train.values)\n",
    "X_test_embs  = sent2vec_model.embed_sentences(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_embs), type(X_train_embs[0]), len(X_train_embs[0]), type(X_train_embs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, now we have 600-dim vectors for every sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=20, learning_rate=1e-2, depth=6, loss_function='Logloss')\n",
    "model.fit(X_train_embs, y_train)\n",
    "preds_class = model.predict(X_test_embs)\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=20, learning_rate=1e-1, depth=6, loss_function='Logloss')\n",
    "model.fit(X_train_embs, y_train)\n",
    "preds_class = model.predict(X_test_embs)\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=20, learning_rate=1, depth=6, loss_function='Logloss')\n",
    "model.fit(X_train_embs, y_train)\n",
    "preds_class = model.predict(X_test_embs)\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(iterations=20, learning_rate=10, depth=6, loss_function='Logloss')\n",
    "model.fit(X_train_embs, y_train)\n",
    "preds_class = model.predict(X_test_embs)\n",
    "np.mean(preds_class == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
